<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>提示词工程PLUS</title>
    <link href="/2024/08/19/%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8BPLUS/"/>
    <url>/2024/08/19/%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8BPLUS/</url>
    
    <content type="html"><![CDATA[<h3 id="prompt"><a href="#prompt" class="headerlink" title="prompt"></a>prompt</h3><p>当前大模型是没有“心算”能力的，无法探知自然语言背后的“隐含信息”，对于中文隐含信息量又是及其庞大的。但我们可以通过提示词让模型将心算能力摆在前台，通过上下文将隐含信息摆在前台。<br><strong>举个例子：</strong></p><ul><li><p>36 * 12等于几，人类通过心算时大部分会拆成36<em>10&#x3D;360放在心里某个位置，然后在算36</em>2&#x3D;72放在心里某个位置，最终相加得出结果，而模型没有地方可以放这个临时值</p><ul><li>这种情况下可以在提示词上说：请分步给出答案，通过step by step的方式让模型有放临时值的“地方”</li></ul></li><li><p>“老王儿子明天摆宴席，让我上台讲两句，我该说点什么词？”，在这个提问背后如果是人与人之间的沟通它背后隐含着老王儿子和提问者是什么关系、摆的什么宴席（结婚、升学…..）可能马上就知道怎么回事，但是模型是无法探知的</p><ul><li>这种情况下可以在提示词上说：老王儿子是我外甥，明天要摆升学宴，通过给出隐含信息让模型输出结果更精准</li></ul></li></ul><h3 id="ReAct"><a href="#ReAct" class="headerlink" title="ReAct"></a>ReAct</h3><p>ReAct 是 Reasoning 和 Acting 的缩写。这个框架的基本思想是给一个 Prompt，这个 Prompt 将 Question 拆成几个步骤。分别是：</p><ul><li><strong>Tought</strong>： 面对这个 Question 我下一步应该做什么。</li><li><strong>Action</strong>：执行某个动作。在 ReAct 里有三种动作，第一个是 Search[entity] 如果存在对应实体的维基页面，则返回前5句话，否则使用<a href="https://www.zhihu.com/search?q=%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2984412161%7D">维基百科</a>搜索引擎搜索前5个类似的实体；第二个是 Look[string] 它将返回包含该字符串的页面中的下一句话，模拟浏览器上的 Ctrl+F 功能。第三个是 Finish[answer] 它将使用答案完成当前任务。</li><li><strong>Observation</strong>：观察到的外部工具给到的结果，将作为新的提示输入给 ChatGPT</li></ul><h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><p>[基于embeddings+GPT的Q&amp;A探索]</p><h3 id="Langchain"><a href="#Langchain" class="headerlink" title="Langchain"></a>Langchain</h3>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>uplift与因果推断</title>
    <link href="/2024/07/30/uplift%E4%B8%8E%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD/"/>
    <url>/2024/07/30/uplift%E4%B8%8E%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD/</url>
    
    <content type="html"><![CDATA[<p>在通常的预测任务中，我们拟合的实际是Y与X的相关关系，X甚至可以是Y的结果，如GDP和发电量之间可能有一系列复杂的关系，但只要二者相关就可以互相预测。在另一些场景中则有所区别，如预测任务要指导干预(Treatment)决策时，我们所能掌控的只有Treatment变量，此时我们希望知道的是执行干预与否的效果差异(通常看增量，uplift)，目的是决策是否执行或执行何种干预。如在“发券&amp;下单”的问题中，用户的历史订单数对下单率预估有较大帮助，但对是否发券的指导意义可能会大打折扣。</p><h2 id="相关、因果、辛普森悖论"><a href="#相关、因果、辛普森悖论" class="headerlink" title="相关、因果、辛普森悖论"></a>相关、因果、辛普森悖论</h2><p>因果关系要求“原因”先于并导致“结果”，而相关关系对顺序不做要求。参考材料中提到了很多示例，如“溺水死亡人数与冰激凌销量正相关”，显然二者不是因果关系，而是由“气温(或季节)”联系起来的相关关系。<br><strong>辛普森悖论(Simpson Paradox)</strong><br>趋势出现在几组数据中，但当这些组被合并后趋势消失或反转。<br>案例：<br>总共的志愿者有700个人（相当于小白鼠），分为两个组，第一组给350个人服用新生产的药物，第二组给另外350个人不用药物（或者说服用糖之类的东西，俗称安慰剂）。服药的第一组350个人中，男性患者87位，女性患者350-87 &#x3D;263位。未服药的第二组350个人中，男性患者270位，女性患者350-270&#x3D;80位，实验结果如表所示<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20240730165623.png" alt="image.png"><br>总数据中，服药的患者痊愈率是78%，未服药的患者痊愈率是83%<br>但是在服药的患者中，男性患者的痊愈率是93%，而未服药的男性患者痊愈率是87%，证明药物有效；女性服药的患者痊愈率是73%，而未服药的患者痊愈率是69%，药物同样有效。<br>整体的效果而言，竟然是不服药的效果好。<br><strong>从数据的角度来说</strong>，此次服用药物患者的350人和不服用药物的350人中，男女比例是不一样的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>DS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>漫谈大模型中的训练</title>
    <link href="/2024/07/19/%E6%BC%AB%E8%B0%88%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%AE%AD%E7%BB%83/"/>
    <url>/2024/07/19/%E6%BC%AB%E8%B0%88%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%AE%AD%E7%BB%83/</url>
    
    <content type="html"><![CDATA[<h3 id="pretraining，SFT-RLHF"><a href="#pretraining，SFT-RLHF" class="headerlink" title="pretraining，SFT, RLHF"></a>pretraining，SFT, RLHF</h3><p>1.Pretraining:第一阶段预训练所需的语料是各种类型的世界知识，包括网页、书籍、新闻、论文期刊、对话文本、代码等形式，通过大量学习世界知识，构建模型的基础能力，使得模型能够“漂亮地说话”。该阶段的语料特征可以概括为“广“。给模型海量的文本进行训练，99%的计算量花费在这个阶段，输出的模型叫做base model，能做的事情就是像成语接龙一样不断的完成一段话；2.SFT(Supervised Fine-tuning):第二阶段SFT,通过标注人员设计问答，编写正确答案，将例题投喂给模型，并希望模型在没有见过的任务中”举一反三”，提升泛化能力。通俗的讲，就是人工介入，给出高质量的文本问答例子。经过问答式训练的Mode叫做SFT model,就可以正常回答人的问题了；<br>3.RLHF(Reinforcement Learning from Human Feedback):第三阶段RLHF,训练目标是让模型的价值观与人类对齐，需要人类对模型的回答进行打分、排序，让模型知道”怎么说更好”。第二和第三阶段的数据质量要求较高，需要来自人类的高质量反馈，语料特征可以概括为“齐“。第三阶段人工先介入，通过对同一个Prompt生成答案的排序来训练一个Reward Model。.再用Reward Model:去反馈给SFT Model,,通过评价生成结果的好坏，让模型更倾向于生成人们喜好的结果。最终生成的Modell叫做RLHF model,</p><p><a href="https://bce.bdstatic.com/p3m/common-service/uploads/SFT-8_fec7948.mp4">https://bce.bdstatic.com/p3m/common-service/uploads/SFT-8_fec7948.mp4</a><br><a href="https://www.youtube.com/watch?v=bZQun8Y4L2A">https://www.youtube.com/watch?v=bZQun8Y4L2A</a><br><a href="https://www.youtube.com/watch?v=zjkBMFhNj_g&t=937s">https://www.youtube.com/watch?v=zjkBMFhNj_g&amp;t=937s</a></p><p>数据长度95%以上在4k以内：4k版本<br>超过4k的数据较多（20%以上）或需长文本处理：8k版本</p><p>全量更新：更新模型所有参数<br>lora：一小部分参数调整<br>prompt tuning：预训练参数不变，优化提示词</p><p>样本数量少于1000且需注重大模型本身的通用能力：lora<br>特定任务数据样本较多且主要关注这些特定任务的效果：全量</p><p>llama-2:<br>epoch：100条数据epoch为15，1000条为10，10000条为2（特殊任务，如自然语言生成sql除外）<br>batchsize越大越好</p><p>如果模型只会回答训练见过的内容，可能是过拟合<br>成功的训练一般training loss和perlexity有明显收敛过程，出现在后半部分，下降且平稳</p><p>rouge<br>bleu<br>看答案的文本相似度，在简单任务中，回答准会比较高，100左右；复杂任务如写故事，50-60就很高了。</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型实战-函数调用</title>
    <link href="/2024/07/13/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98-%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8/"/>
    <url>/2024/07/13/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98-%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb">https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb</a><br>Chat聊天：纯粹的Chat,像是一个主要由“大脑和嘴”构成的智能体，专注于信息处理和语言交流。比如ChatGPT这样的系统，它能够理解用户的查询，给出有用和连贯的回答，但它本身不直接执行任务。<br>Agent代理：像一个具有“手、脚”的智能体，它能够进行思考、决策，并且能执行具体的任务</p><p>总结一下，这里面的流程步骤：<br>1.用户把问题和函数功能交给openai<br>2.openai通过json格式返回能处理的问题（比如，一个模糊的地点，转换为准确的城市和<br>省)<br>3.用户通过返回的具体答案，调用自己的数据库，或是第三方api,获取json格式的数据<br>4.把上下文和返回的json数据汇总给openai<br>5.openai通过自然语言返回用户提出的问题</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型实战-文本分类和聚类</title>
    <link href="/2024/07/05/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E5%92%8C%E8%81%9A%E7%B1%BB/"/>
    <url>/2024/07/05/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E5%92%8C%E8%81%9A%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/629024155">https://zhuanlan.zhihu.com/p/629024155</a></p><h3 id="openai的completion与embedding接口"><a href="#openai的completion与embedding接口" class="headerlink" title="openai的completion与embedding接口"></a>openai的completion与embedding接口</h3><p>1.Completion可以让模型根据用户的输入进行目动续写，Completion接口，一万面可以直接当作天机器人使用，另一方面，只要善用prompt提示词，就能完成文案撰写、文本摘要、机器翻译等工作2.Embedding可以将用户输入的文本转化成向量。Embedding向量适合作为一个中间结果，用于传统的机器学习场景，比如分类、聚类。</p><p>OpenAI 支持批量调用接口，可以在一个请求里一次批量处理很多个请求。通过将 1000 条记录打包在一起处理，速度将会快很多。<br>对于大数据集，我们不应该存储成 CSV 格式。特别是获取到的 Embedding 数据，它是由很多浮点数组成的。如果存储成 CSV 格式，那么它会把本来只需要 4 个字节的浮点数都用字符串的形式存储下来，这会浪费好几倍的空间，并且写入的速度也会变得很慢。因此，我在这里采用了 parquet 这个序列化的格式。使用 parquet 格式可以节省空间并提高写入速度，整个存储的过程只需要 1 分钟左右。另外，为了确保数据的安全，我们还可以对 parquet 文件进行加密和压缩，这样可以进一步减小存储空间，并且保护数据的机密性。</p><h3 id="指标含义"><a href="#指标含义" class="headerlink" title="指标含义"></a>指标含义</h3><p>准确率表示模型正确判断为该类别的标题所占的比例，即在所有判断为该类别的标题中有多少是真正属于该类别。举个例子，模型判断有100个标题属于农业新闻，但实际上只有83个标题是农业新闻，那么准确率就是0.83。准确率越高越好，但是并不意味着准确率达到100%就代表模型完全正确，因为模型可能会漏判，所以我们还需要考虑召回率。</p><p>召回率表示模型正确判断为该类别的标题占实际该类别下所有标题的比例，即没有漏掉的比例。例如，模型判断有100个标题属于农业新闻，这100个标题实际上都是农业新闻。准确率已经达到100%，但是实际上我们共有200条农业新闻。因此，在农业新闻类别中，我们的召回率只有100&#x2F;200 &#x3D; 50%。</p><p>因此，评估模型效果时需要考虑准确率和召回率，综合考虑这两个指标得出的结果就是F1分数。F1分数是准确率和召回率的调和平均数，即 F1 Score &#x3D; 2 * (Precision * Recall) &#x2F; (Precision + Recall)。当准确率和召回率都为100%时，F1分数也为1。如果准确率为100%，召回率为80%，那么计算得到的F1分数为0.88。F1分数越高越好。</p><p>支持样本量表示数据中实际属于该类别的样本数量。一般来说，样本数量越多，该类别的训练结果就越准确。</p><p>accuracy：总体上判断正确的分类数除以测试样本数，即模型的整体准确率。<br>macro average是宏平均，它将每个类别计算得到的指标加在一起取平均。宏平均对于数据分类不平衡的情况非常有用。比如，假设我们进行情感分析，其中90%的样本属于正面情感，而10%的样本属于负面情感。在这种情况下，如果我们的模型在正面情感方面的预测效果很好，准确率达到了90%，但在负面情感方面的准确率只有50%。如果只看整体数据，准确率似乎很高，因为正面情感的样本很多。但是对于我们的目标来说，即找到具有负面情感的客户并与他们沟通、进行赔偿，整体准确率就没有什么用了。而宏平均会将整体准确率计算为(90% + 50%)&#x2F;2 &#x3D; 70%。这并不是一个很好的预测结果，我们需要进一步优化模型。宏平均在处理数据样本不平衡的情况下非常有用</p><h3 id="文本聚类"><a href="#文本聚类" class="headerlink" title="文本聚类"></a>文本聚类</h3><p>K-Means:是一种无监督的聚类算法，其中的K代表类簇个数，Means代表类簇内数据对象的均值。算法基本原理是：先从样本集中随机选取K个样本作为簇中心，并计算所有样本与这K个“簇中心”的距离；对于每一个样本，将其划分到与其距离最近的“簇中心”所在的簇中；然后再计算每个簇的新的“簇中心”。迭代进行上述过程，直到达到某个终止条件（如迭代次数、簇中心的移动距离小于某个阈值等）为止。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">1</span> <span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-number">2</span> <span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans<br><span class="hljs-number">3</span><br><span class="hljs-number">4</span> embedding_df pd.read_parquet(<span class="hljs-string">&quot;data/20newsgroups.parquet&quot;</span>)<br><span class="hljs-number">5</span><br><span class="hljs-number">6</span> matrix np.vstack(embedding_df.embedding.values)<br>7num_of_clusters =<span class="hljs-number">20</span><br><span class="hljs-number">8</span><br><span class="hljs-number">9</span> kmeans KMeans(n_clusters=num_of_clusters,init=<span class="hljs-string">&quot;k-means++&quot;</span>,n_init=<span class="hljs-number">10</span>, random_state=<span class="hljs-number">42</span>)<br><span class="hljs-number">10</span> kmeans.fit(matrix)<br><span class="hljs-number">11</span> labels kmeans.labels_<br><span class="hljs-number">12</span> embedding_df[<span class="hljs-string">&quot;cluster&quot;</span>]=labels<br><span class="hljs-number">13</span><br><span class="hljs-number">14</span><span class="hljs-comment">#统计每个clusteri的数量</span><br><span class="hljs-number">15</span> new_df embedding_df.groupby(<span class="hljs-string">&#x27;cluster&#x27;</span>)[<span class="hljs-string">&#x27;cluster&#x27;</span>].count().reset_ind<br><span class="hljs-number">16</span><span class="hljs-comment">#输出结果</span><br><span class="hljs-number">17</span> <span class="hljs-keyword">from</span> IPython.display <span class="hljs-keyword">import</span> display<br><span class="hljs-number">18</span> display(new_df)<br></code></pre></td></tr></table></figure><p>上述代码通过numPy的stack函数，把所有的Embedding)放到一个矩阵里面，并设置了要聚<br>合出来的类的数量，最后调用了一下K-Means:算法的ft方法。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>token介绍</title>
    <link href="/2024/06/30/token%E4%BB%8B%E7%BB%8D/"/>
    <url>/2024/06/30/token%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p><a href="https://platform.openai.com/tokenizer">https://platform.openai.com/tokenizer</a><br>OpeA的大型语言模型（即GPT)使用标记处理文本，标记是一组文本中常见的字符序列。模型学习理解这些标记之间的统计关系，并擅长生成标记序列中的下一个标记。您可以使用相关工具来了解语言模型如何对一段文本进行标记，以及该段文本中标记的总数</p><h3 id="token是什么"><a href="#token是什么" class="headerlink" title="token是什么"></a>token是什么</h3><p>在自然语言处理(NLP)中，token是指一组相关的字符序列，例如一个单词或一个标点符号。将文本分解为token是NLP的一项基本任务，因为它是许多其他任务的先决条件，例如词性标注、命名实体识别和机器翻译。在文本处理中，token可以是一个词语、数字、标点符号、单个字母或任何可以成为文本分析的单个元素。</p><p>在分解文本时，通常会根据空格、标点符号和其他特定的分割符号来确定token的边界。<br>例如，在以下句子中，标点符号和空格用于分解成为不同的token:<br>“我喜欢吃冰淇淋。”<br>这个句子中，每个汉字和标点符号都可以切分开成一个tokn。但是，一个字一个字去理解整句话的意思，可能反而会理解错误。<br>例如“冰淇淋”，就是一个完整的词，分开成“冰”“淇”“淋”三个字反而无法理解了。<br>类似的，NLP中，token还可以是比词更高级别的语言单位，例如短语或句子。例如，对于短语token,“红色的苹果”可以被视为一个token,而不是单独的“红色”和“苹果”token.。因为存在不同的切分方式，所以“红色的苹果”，就需要切分成“红”“红色”的”苹果””“果”“红色的苹果”等多个token:去理解。<br>在处理文本时，理解token的概念是非常重要的，因为它是许多文本分析任务的基础。NLP算法会使用token来构建文本的表示形式，理解自然语言，以便进行其他分析任务。<br>因此，对于NLP系统来说，选择正确的分词方法(tokenization)非常重要，它将直接影响到其他任务的准确性和效率。<br>我们结合Chatgpt进一步了解下token和tokenization。</p><h3 id="ChatGPT中的token"><a href="#ChatGPT中的token" class="headerlink" title="ChatGPT中的token"></a>ChatGPT中的token</h3><p>一句话输入大模型之后后先拆分成token，ChatGPT的词表一共有100256个不同符号（BPE格式）。<br>常见单词对应的token索引id小。如果一个单词单独出现时被拆分成了两个token，但是前面加上空格后可以分配到一个token，说明该单词在段落中间出现的概率高。<br>token 其实还包括了一些空白字符，因此在边界容易出问题，这是在应用时需要注意的。<br>以前面「Once upon a time」的例子说明，”Once “（后面有空格）的 token id 是 <code>[7454, 220]</code>，其中空格的 id 是 220，但如果是 <code>&quot;Once upon&quot;</code>，token id 就是 <code>[7454, 2402]</code>，其中 <code>&quot; upon&quot;</code>（前面有空格） 的 id 是 2402，而 “upon” 这个不加前面空格的单词 token 是 27287，前面提到过 id 值大意味着概率低，在大模型眼里，提示词结尾加不加空格是完全不同，加了就是用 <code>[7454, 220]</code> 预测 27287，不加就是用 <code>[7454]</code> 预测 2402，第一种是概率更低更难的，因此在写提示词的时候<strong>结尾不要加空格</strong></p><p><a href="https://www.youtube.com/watch?v=zduSFxRajkE">https://www.youtube.com/watch?v=zduSFxRajkE</a></p><p>看完了 token，接下来介绍选词，前面提到大模型最终输出结果是每个 token 的概率，要选择哪个 token 作为下一个词呢？最简单的想法是选择概率最大的，但这样并不好，一方面是单次概率最大不代表全局最优，另一方面导致大模型同一个问题每次输出结果都一样，对于创意类的场景不合适。因此目前的做法是根据输出概率来做采样，概率高的 token 更容易输出。</p><p>在实际应用的时候，大模型通常会有个可调整的参数叫 <code>Temperature</code>，它可以控制大模型输出结果更稳定还是更多样，它是怎么实现的呢？这里用代码来解释一下，比如我们假设大模型输出了这个向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">output = torch.FloatTensor([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.5</span>])<br></code></pre></td></tr></table></figure><p>通过 softmax 函数我们可以将输出结果转成总和为 1 的小数，每个小数就是输出概率，这就是大模型最后算出的结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.nn.functional.softmax(output, -<span class="hljs-number">1</span>) <br><span class="hljs-comment"># 输出为 [0.1621, 0.1792, 0.1980, 0.2188, 0.2419]</span><br></code></pre></td></tr></table></figure><p>比如第五个值被选中的概率是 24.19%，这里每个数组的索引就是前面提到的 token id，第五个值代表 token id 为 5。</p><p>如果我们将输出结果除以 0.1，结果就变成如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">temperature = <span class="hljs-number">0.1</span><br>torch.nn.functional.softmax(output / temperature, -<span class="hljs-number">1</span>)<br><span class="hljs-comment"># 输出为 [0.0117, 0.0317, 0.0861, 0.2341, 0.6364]</span><br></code></pre></td></tr></table></figure><p>这时第五个值的概率变成 63.64% 了，被选中的概率大幅增加，输出结果更为稳定</p><p>而如果是除以 2，就变成：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">temperature = <span class="hljs-number">2</span><br>torch.nn.functional.softmax(output / temperature, -<span class="hljs-number">1</span>)<br><span class="hljs-comment"># 输出为 [0.1805, 0.1898, 0.1995, 0.2097, 0.2205]</span><br></code></pre></td></tr></table></figure><p>这时各个列的输出概率就被压平了，第五个值的的概率变成 22%，而之前第一个值的概率从 1% 变成了 18.05%，比之前更有可能被选中了，这就使得大模型输出结果更多样，也意味着跟容易瞎说。<br>因此将 <code>temperature</code> 值设小一点模型就能很稳定输出，另外 <code>temperature</code> 是被除数，所以不可以为 0，有些平台支持 0 是做了特殊处理，比如可以转成 <code>top_k</code> 为 1。</p><p>不过改成 0 也不能完全保证结果唯一，根据 OpenAI 员工 <a href="https://link.zhihu.com/?target=https://community.openai.com/t/a-question-on-determinism/8185">boris</a> 的说法，GPU 浮点数计算的时候有不确定性，而且多个 GPU 推理时 <code>a*b*c</code> 可能被计算为 <code>(a*b)*c</code> 或 <code>a*(b*c)</code>，这两个结果可能会微小不同，导致最终结果不唯一。</p><p>前面花了很多篇幅介绍 token 及选词策略，因为这是在大模型应用时控制输出的重要机制，在不改变大模型参数的情况下，我们可以：</p><ol><li>提升推理速度，比如 <a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2302.01318">Speculative Sampling</a>，它的原理是用个小模型来生成一段，再让大模型来挑选是否可用，这个方式能提升 2-3 倍推理速度。</li><li>优化推理效果，比如使用 <a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Beam_search">Beam search</a> 来搜索更好的结果，它通过多次搜索来找到更优结果，但会牺牲性能，因此实际应用中比较少见。</li><li>控制输出格式，在应用中我们通常需要大模型输出 JSON 格式，而大模型有时候会幻觉导致输出错误，这时可以在解码时进行干预，比如发现新 token 会导致 JSON 语法错误就自动修正，比起全部输出后再修正效果更好。</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GPT名称含义</title>
    <link href="/2024/06/25/GPT%E5%90%8D%E7%A7%B0%E5%90%AB%E4%B9%89/"/>
    <url>/2024/06/25/GPT%E5%90%8D%E7%A7%B0%E5%90%AB%E4%B9%89/</url>
    
    <content type="html"><![CDATA[<h3 id="generative"><a href="#generative" class="headerlink" title="generative"></a>generative</h3><p>Generative这个单词，揭示了Chat Gpt的本质就是一个词语接龙器。<br>比如，问：今天天气很糟。<br>GPT的工作是步骤是：<br>1)检查句子是不是完整。假如我问的是：“今年天气很”，那么GPT的回答很可能是：很抱<br>歉，您的信息不完整。<br>2)把“今天天气很糟”作为输入，从众多中文字里面选出一个频率最高的单词，这里是“很”；<br>3)然后把“今天天气很糟，很作为输入，得到今天天气很糟，很遗憾”；<br>4)重复2、3两步，直到碰到一个“END”结束。<br>GPT一定是一个字一个字崩出来的，我们在网页版的时候会看到答案输出是一个字一个字来<br>的。<br>GPT按照事前设置的模型，套出后文，而这个模型就是pre-trained出来的。</p><h3 id="pre-trained"><a href="#pre-trained" class="headerlink" title="pre-trained"></a>pre-trained</h3><p>预训练又称为学习，有三种模式：无监督学习、监督学习、增强学习。<br>GPT起先是无监督学习，就是喂给机器各种文本，在1.0，喂的数据是1GB;到了2.0这个数<br>据是40GB;而到了GPT3,是570GB。<br>无监督学习的目的，就是根据“公式”算出下一个出现的词的概率，然后选择排名靠前的词。<br>既然公式，那就必然有参数，比如，f(x)&#x3D;ax+b,这里的a和b就是参数。我们知道，语言是<br>一个很复杂的表达，根据前文要能算出后文，这个公式就会很复杂，牵涉到的参数会很多。<br>具体是：GPT1.0117M参数，到了2.0是1542M参数，而到了GPT3是175B的参数。理论上<br>讲，参数越多，输出的结果就越准确。<br>但是，仅仅是无监督学习是完全不够的。<br>比如：<br>如果问7+9&#x3D;，机器很可能会输出（)，因为，它接受的训练里面，大量的句子就是7+9&#x3D;<br>()<br>很正确，但是这不是我们想要的答案。<br>这时候就需要引入“导师”，告诉它这个答案不对，应该回答16，这需要人工干预，纠正机器<br>的错误。<br>当然，这个所谓的“人工干预”，通常不是“人肉干预”，会有另外一段代码去干预，比如的计<br>算器。这个过程就是监督学习了。<br>对于有确定答案的问句，监督学习当然很好。但是有些问题是没有标准答案的，比如：<br>给我写首李白风格的诗。<br>这时候需要“增强学习”，增强学习告诉机器的不是正确答案，而是给机器点赞。</p><h3 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h3><p>语言，其实就是一个文字序列。<br>所以，问答，就是从一个文字序列到另一个文字序列。人工智能里面有个大名鼎鼎的词：<br>Seq2Seq,说得就是这个算法。<br>两个句子体会下：<br>句子A:小马没能瞠过河流，因为它太累了；<br>句子B:小马没能蹚过河流，因为它太宽了；<br>显然，第一个句子中的它是小马，而第二个句子中的它是河流。<br>一个词的含义和它的前后文都有关系，为了处理这个东西，起先是用的RNN算法循环神经<br>网络(Recurrent Neural Network,.RNW)。各位大佬前赴后继，一通折腾，总算算法搞的<br>比较好了。<br>RNN是个没法并行计算的网络架构，前一个算完了才能算后一个，它唯一问题就是：这玩<br>意很难并行计算。不能并行计算就没多大价值，因为太慢了。<br>直到有一篇论文横空出世，论文题目也是相当霸道，《Attention Is All You Need)》。<br>绝对的超级大招，彻底解决了这个问题。这个模型就是Transformer.。Transformer中存在6<br>层的encode.与6层的decode,每一个decode都直接跟encode连接上，那么6个decode可以<br>直接并行计算，在decode层面速度就可以提升6倍。而GPU可以支持并行处理。<br><a href="https://www.youtube.com/watch?v=zxQyTK8quyY">https://www.youtube.com/watch?v=zxQyTK8quyY</a></p><h3 id="术语整理"><a href="#术语整理" class="headerlink" title="术语整理"></a>术语整理</h3><ol><li>in-context learning 上下文学习<br>在prompt里面写几个例子，模型就可以照着这些例子做生成，即在给定的上下文中使用新的输入来改善模型的输出。这种学习方式并不涉及到梯度更新或微调模型的参数，而是通过提供一些具有特定格式或结构的示例输入，使模型能够在生成输出时利用这些信息。例如，如果你在对话中包含一些英法翻译的例子，然后问模型一个新的翻译问题，模型可能会根据你提供的上下文示例生成正确的翻译</li><li>few-shot learning 少样本学习<br>用极少量的标注样本来训练机器学习模型的技术。在GPT-3的案例中，少样本学习的实现方式是向模型提供少量的输入输出对示例，这些示例作为对话的一部分，描述了模型应该执行的任务。然后模型会生成一个输出，该输出是对与示例类似的新输入的响应。例如，你可以给模型提供几个英法翻译的例子，然后给出一个新的英文单词让模型翻译，模型会尝试产生一个正确的翻译</li><li>prompt engineering 提示工程<br>提示工程是指设计和优化模型的输入提示以改善模型的输出。在大型语言模型中，如何提问或构造输入的方式可能对模型的输出有重大影响。因此，选择正确的提示对于获取有用的输出至关重要。例如，为了让GPT3生成一个诗歌，你可能需要提供一个详细的、引导性的提示，如“写一首关于春天的十四行诗，而不仅仅是“写诗</li><li>prompt 提示词<br>把prompt输入给大模型，大模型给出completion</li><li>instruction tuning 指令微调<br>用instruction 来fine-tune大模型</li><li>code tuning 代码微调<br>用代码来fine-tune大模型</li><li>reinforcement learning with human feedback(RLHF) 基于人类反馈到强化学习<br>让人给模型生成的结果打分，用人打的分来调整模型</li><li>chain of thought 思维链<br>在写prompt时，不仅给出结果，还要一步一步地写结果时怎么推出来的</li><li>scaling laws 缩放法则<br>模型的效果的线性增长要求模型的大小指数增长</li><li>alignment 与人类对齐<br>让机器生成符合人类期望的，符合人类价值观的句子</li><li>emergent ability 突现能力<br>小模型没有，只有模型达到一定规模才会出现的能力</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>embedding</title>
    <link href="/2024/06/24/embedding/"/>
    <url>/2024/06/24/embedding/</url>
    
    <content type="html"><![CDATA[<p>将高维数据映射到低维空间。Embedding的最大价值是降维：在许多实际问题中，原始数据的维度往往非常高。例如，在自然语言处理中，如果使用One-hot编码来表示词汇，其维度等于词汇表的大小，可能达到数十万甚至更高。通过Embedding,我们可以将这些高维数据映射到一个低维空间，大大减少了模型的复杂度。</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>预训练语言模型</title>
    <link href="/2024/06/19/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    <url>/2024/06/19/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h3 id="NLP语言模型发展一览"><a href="#NLP语言模型发展一览" class="headerlink" title="NLP语言模型发展一览"></a>NLP语言模型发展一览</h3><p>深度学习阶段：encoder-decoder&#x2F;word2vec&#x2F;attention<br>预训练模型阶段：transformer&#x2F;bert&#x2F;GPT-2&#x2F;GPT-3，pre- training+fine-tuning<br>大语言模型阶段：GPT3.5&#x2F;GPT4，instruction-tuning&#x2F;prompt-training</p><pre><code class="hljs">深度学习的训练：inference得到预测值与标注数据对比，使用loss function得到差，使用back propagation修改。过程中进行梯度下降（SGD）</code></pre><p>预训练语言模型首先使用大量未标注数据进行自监督学习，学习出的语言模型针对特定的下游任务进行微调。</p><h3 id="prompt-tuning和fine-tuning的区别"><a href="#prompt-tuning和fine-tuning的区别" class="headerlink" title="prompt tuning和fine tuning的区别"></a>prompt tuning和fine tuning的区别</h3><p>prompt tuning会固定预训练模型的参数<br>Prompt Tuning和Fine Tuning虽然都是微调，但是有区别。提示学习(PromptTuning)的灵感源于GPT-3,就是在Prompt中插入一段task specific的可以tune的prompt token。由于这个token对于每个任务都是不同的，所以可以帮助机器识别任务到底是什么。又因为机器自己学习(tune)这个prompt token,所以这个token对于机器会有非常好的效果。</p><p>提出Prompt Tuning的动机是，语言模型（Language Models）越来越大，Fine-tune的成本也越来越高。<br>Fine-tuning的本质是改变预训练模型的weights。由于预训练模型在原域上已经有非常好的性能了，域的迁移会受到原域的阻力，因为使用fine-tune改变的weight是原域上的weight。这样，想要在新域上获得较好的性能，预训练模型越大，fine-tune需要的数据也越多。</p><h4 id="prompt的分类"><a href="#prompt的分类" class="headerlink" title="prompt的分类"></a>prompt的分类</h4><h5 id="硬提示-离散提示（Hard-Prompt-Discrete-Prompt）"><a href="#硬提示-离散提示（Hard-Prompt-Discrete-Prompt）" class="headerlink" title="硬提示&#x2F;离散提示（Hard Prompt&#x2F;Discrete Prompt）"></a>硬提示&#x2F;离散提示（Hard Prompt&#x2F;Discrete Prompt）</h5><p>人为设计<br>根据2021年的两份研究，硬提示有两个性质：</p><ul><li>人类认为不错的硬提示对于LM来说不一定是一个好的硬提示，这个性质被称为硬提示的sub-optimal（次优）性。</li><li>硬提示的选择对于预训练模型的影响非常大。</li></ul><h5 id="软提示-连续提示（Soft-Prompt-Continuous-Prompt）"><a href="#软提示-连续提示（Soft-Prompt-Continuous-Prompt）" class="headerlink" title="软提示&#x2F;连续提示（Soft Prompt&#x2F;Continuous Prompt）"></a>软提示&#x2F;连续提示（Soft Prompt&#x2F;Continuous Prompt）</h5><p>把Prompt的生成本身作为一个任务进行学习，相当于把Prompt的生成从人类一个一个尝试（离散）变换成机器自己进行学习、尝试（连续）。由于需要机器自己学习，软提示不可避免地往模型内引入了新的参数。这里就又出来一个问题：如何<strong>参数有效</strong>地学习软提示？目前的研究热点有：</p><ul><li>P-tuning：将prompt变成token，用BiLSTM进行学习。</li><li>P-tuning：使用混合的prompt初始化策略（如CLInit和SelectInit）。</li><li>Prefix-tuning：对于不同模型，将prompt注入输入的不同位置。</li></ul><h4 id="从Prompt到Prompt-Tuning：让机器学习写Prompt"><a href="#从Prompt到Prompt-Tuning：让机器学习写Prompt" class="headerlink" title="从Prompt到Prompt Tuning：让机器学习写Prompt"></a>从Prompt到Prompt Tuning：让机器学习写Prompt</h4><p><strong><em>所谓Prompt Tuning，就是在Prompt中插入一段task-specific的可以tune的prompt token。</em></strong><br>由于这个token对于每个任务都是不同的，所以可以帮助机器识别任务到底是什么。又因为机器自己学习（tune）这个prompt token，所以这个token对于机器会有非常好的效果。</p><h5 id="参数有效性（Parameter-efficiency）"><a href="#参数有效性（Parameter-efficiency）" class="headerlink" title="参数有效性（Parameter-efficiency）"></a>参数有效性（Parameter-efficiency）</h5><p>所谓“参数有效”，本质就是“节约参数”。怎样算节约参数呢？复用预训练模型越多，节约的参数越多。这是因为Fine-tune预训练模型相当于改变整个预训练模型，而这需要改变非常多的参数。想要进行Transfer Learning的话，改变这么多参数是不能接受的。因此，Prompt Learning的一大好处就叫Parameter Efficient。</p><h3 id="预训练模型三种网络架构"><a href="#预训练模型三种网络架构" class="headerlink" title="预训练模型三种网络架构"></a>预训练模型三种网络架构</h3><p>1.Encoders编码器：编码器主要用于处理和理解输入信息。这些模型可以获得双向的上下文，即可以同时考虑一个词的前面和后面的信息。由于编码器可以考虑到未来的信息，它们非常适合用于需要理解整个句子的任务，如文本分类、命名实体识别等。预训练编码器需要使其能够构建强大的表示，这通常通过预测某个被遮蔽的单词、预测下一个句子或者其它的预训练任务来实现。BERT就是一种典型的预训练编码器，如RoBERTa、ALBERT。百度的ERNIE也跟BERT类似。<br>2.Decoders解码器：解码器主要用于生成输出信息。它们是语言模型的基础，用于预测下一个单词。解码器只能考虑到过去的词，而不能考虑到未来的词，这使得它们非常适合于生成任务，如文本生成、对话系统等。GPT就是一种典型的预训练解码器。如GPT-1、GPT-2、GPT-3三代。所以说GPT名称叫作预训练的解码器也是可以的。<br>3.Encoders-Decoders编码器-解码器：编码器-解码器结构结合了编码器和解码器的优点。编码器首先处理输入信息，然后解码器生成输出信息。这种结构可以考虑到全局的上下文信息，因此非常适合于需要理解输入信息并生成相应的Decoders输出的任务，如机器翻译、文本摘要等。如何预训练编码器解码器模型是一个开放的问题，因为需要考虑到如何有效地使用编码器和解码器的特点。T5和BART都是典型的预训练编码器解码器模型。</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
