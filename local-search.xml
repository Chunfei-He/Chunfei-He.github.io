<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>GPTQ：从量化谈起</title>
    <link href="/2025/08/03/GPTQ%EF%BC%9A%E4%BB%8E%E9%87%8F%E5%8C%96%E8%B0%88%E8%B5%B7/"/>
    <url>/2025/08/03/GPTQ%EF%BC%9A%E4%BB%8E%E9%87%8F%E5%8C%96%E8%B0%88%E8%B5%B7/</url>
    
    <content type="html"><![CDATA[<p>模型量化是一种压缩网络参数的方式，它将神经网络的参数（weight）、特征图（activation）等原本用浮点表示的量值换用定点（整型）表示，在计算过程中，再将定点数据反量化回浮点数据，得到结果。<strong>模型量化实现建立在深度网络对噪声具有一定的容忍性上</strong>，模型量化相当于对深度网络增加了一定的噪声（量化误差），如果量化位数合适，模型量化基本不会造成较大的精度损失。<br>量化模型实现加速，不仅仅由于整形运算比浮点运算更快，我更倾向于加速了对权重的读取（访存加速），尤其是以大模型LLM的部署为例，Self-Decoder 阶段就是典型的 Memory-Bound 的操作。其余的加速方式，要依赖于量化算子优化、量化图优化等方面。</p><h1 id="量化"><a href="#量化" class="headerlink" title="量化"></a>量化</h1><h2 id="量化对象"><a href="#量化对象" class="headerlink" title="量化对象"></a>量化对象</h2><p>可以对模型参数（weight）、激活值（activation）或者梯度（gradient）做量化。<br>通常而言，模型的参数分布较为稳定，因此对参数 weight 做量化较为容易。<br>然而，模型的激活值往往存在异常值，直接对其做量化，会降低有效的量化格点数，导致精度损失严重，因此，激活值的量化需要更复杂的处理方法（如 SmoothQuant）。<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20250803203949.png" alt="quan_weight.png"></p><h2 id="线性量化下，浮点数与定点数之间的转换公式"><a href="#线性量化下，浮点数与定点数之间的转换公式" class="headerlink" title="线性量化下，浮点数与定点数之间的转换公式"></a>线性量化下，浮点数与定点数之间的转换公式</h2><p><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20250803204519.png" alt="quan_cal.png"></p><ul><li>R 表示量化前的浮点数</li><li>Q 表示量化后的定点数</li><li>S（Scale）表示缩放因子的数值</li><li>Z（Zero）表示零点的数值<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/v2-1ce8aece0beba9dae4e7aaf6a64d4f6d_1440w.png" alt="v2-1ce8aece0beba9dae4e7aaf6a64d4f6d_1440w.png"><br>图中展示的是量化到 int8，因此 Round to Grid 实际上是先乘以 127(&#x3D;2**(8-1)-1)，然后 round 到最近的整数。</li></ul><h2 id="对称量化、不对称量化"><a href="#对称量化、不对称量化" class="headerlink" title="对称量化、不对称量化"></a>对称量化、不对称量化</h2><p><strong>对称量化</strong>（如左图所示）中，量化前后的 0 点是对齐的，因此不需要记录零点。它适合对分布良好且均值为 0 的参数进行量化。因此对称量化常用于对 weight 量化。<br><strong>非对称量化</strong>（如右图所示）中，量化前后 0 点不对齐，需要额外记录一个 offset，也就是零点。非对称量化常用于对 activation 做量化。<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/v2-6f905bb60c96753b5dd82820db5a65d7_1440w.png" alt="v2-6f905bb60c96753b5dd82820db5a65d7_1440w.png"></p><h2 id="PTQ-and-QAT"><a href="#PTQ-and-QAT" class="headerlink" title="PTQ and QAT"></a>PTQ and QAT</h2><ul><li>QAT（Quant-Aware Training） 也可以称为<strong>在线量化（On Quantization）</strong>。它需要利用额外的训练数据，在量化的同时结合反向传播对模型权重进行调整，意在确保量化模型的精度不掉点。</li><li>PTQ （Post Training Quantization）也可以称为<strong>离线量化（Off Quantization）</strong>。它是在已训练的模型上，使用少量或不使用额外数据，对模型量化过程进行校准，可能伴有模型权重的缩放。其中：</li><li><strong>训练后动态量化（Post</strong> <strong>Dynamic Quantization）不使用校准数据集，直接对每一层 layer 通过量化公式进行转换。</strong><code>QLoRA 就是采用这种方法。</code></li><li><strong>训练后校正量化（Post Calibration Quantization）需要输入</strong>有代表性<strong>的数据集，根据模型每一层 layer 的输入输出调整量化权重。</strong><code>GPTQ 就是采用这种方法。</code>本质上 PTQ 就是在校准过程中，研究不同的metric来更好地选择截断上下界。例如常见的 MinMax，Histogram，Entropy等，也有一些基于 search 的方法来遍历探索。不同场景不同网络适合的metrci 各不相同，校准集的选择也会对结果有所影响，需要在实际应用中尝试不同的组合来获得最好的效果。<br>在实际应用中，更为常用的是PTQ的方法，大部分芯片厂商自己的编译器，已经集成了基础的PTQ方法，并与算子融合图优化等组合使用，对绝大部分模型能获得精度与速度都令人满意的结果。只有少部分特殊的模型结构场景，或更低bit量化的量化需求情况下才会考虑QAT的方法。</li></ul><h1 id="GPTQ-Accurate-Post-Training-Quantization-for-Generative-Pre-trained-Transformers"><a href="#GPTQ-Accurate-Post-Training-Quantization-for-Generative-Pre-trained-Transformers" class="headerlink" title="GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"></a>GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</h1><p>GPTQ 对某个 block 内的所有参数逐个量化，每个参数量化后，需要适当调整这个 block 内其他未量化的参数，以弥补量化造成的精度损失。GPTQ 量化需要准备校准数据集。<br>GPTQ 的思想最初来源于 Yann LeCun 在 1990 年提出的OBD算法，随后OBS、OBC（OBQ） 等方法不断进行改进，而 GPTQ 是 OBQ 方法的加速版。GPTQ 的量化有严谨的数学理论推导，所有的算法步骤都有理论支撑。为了理解 GPTQ 的思想，我们需要先介绍 OBD -&gt; OBS -&gt; OBQ 的演进过程。</p><h2 id="OBD：Optimal-Brain-Damage"><a href="#OBD：Optimal-Brain-Damage" class="headerlink" title="OBD：Optimal Brain Damage"></a>OBD：Optimal Brain Damage</h2><p>关于神经网络剪枝算法的论文，主要利用二阶导数信息度量模型参数的显著性，也就是度量模型删除参数对模型结果的影响，剪掉影响小的参数降低模型复杂度提高泛化能力。<br>具体算法流程为</p><ol><li>搭建神经网络</li><li>训练神经网络至损失函数收敛</li><li>计算神经网络每个参数的二阶导数</li><li>计算神经网络每个参数的显著性</li><li>按照显著性对参数进行排序，并删除一些低显著性的参数。可认为删除参数是将其设置为0并训练时冻结。</li><li>从步骤2 开始重复<br>该算法需要重复训练、剪枝操作，这同我们理解的训练后大语言模型量化似乎差的很远。但带来了通过神经网络参数二阶导数来评估参数对模型结果影响核心想法。</li></ol><p>构建神经网络目标函数在局部点(参数)上扰动的二阶近似函数。<br>首先在指定参数位置对损失函数做泰勒二阶展开<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20250803205653.png" alt="image.png"><br>参数w变化带来的损失函数扰动表示为<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20250803205704.png" alt="image.png"><br>在OBD论文中这部分公式写成<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20250803205715.png" alt="image.png"><br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20250803205735.png" alt="image.png"><br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20250803205744.png" alt="image.png"><br>于是，上式可以简化成：<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20250803205754.png" alt="image.png"><br>由此可以得到删除一个参数，对目标函数的影响。所以只需要计算海森矩阵，就可以知道每个参数对目标的影响。然后就可以按照影响从小到大给参数排个序，这样就确定了参数剪枝的次序。</p><h2 id="OBS：Optimal-Brain-Surgeon"><a href="#OBS：Optimal-Brain-Surgeon" class="headerlink" title="OBS：Optimal Brain Surgeon"></a>OBS：Optimal Brain Surgeon</h2><p>OBS 认为，参数之间的独立性不成立，我们还是要考虑交叉项，因此上式变成了<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20250803210335.png" alt="image.png"><br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20250803210834.png" alt="image.png"><br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20250803210843.png" alt="image.png"><br>于是，也只需要求解海森矩阵的逆，就可以计算每个参数对目标的影响 ，然后就可以按照影响从小到大给参数排个序，这样就确定了参数剪枝的次序。同时，每次剪枝一个参数，其他的参数也按照δw更新一次。<br><strong>这里的思想一直沿用到了 GPTQ 算法：也就是对某个 block 内的所有参数逐个量化，每个参数量化后，需要适当调整这个 block 内其他未量化的参数，以弥补量化造成的精度损失。</strong></p><h2 id="OBC"><a href="#OBC" class="headerlink" title="OBC"></a>OBC</h2><p>OBD 和 OBS 都存在一个缺点，就是剪枝需要计算全参数的海森矩阵（或者它的逆矩阵）。但在动辄上亿参数的神经网络下求解海森矩阵显然不可能。于是，我们可以假设参数矩阵的同一行参数互相之间是相关的，而不同行之间的参数互不相关，这样，海森矩阵就只需要在每一行内单独计算。<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20250803211044.png" alt="image.png"><br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/v2-d178e091e2068ba9256589f41acfc0d8_1440w.png" alt="v2-d178e091e2068ba9256589f41acfc0d8_1440w.png"><br>从 for 循环开始逐行分析：</p><ul><li>Line 1：找到对目标函数影响最小的参数 p</li><li>Line 2：对参数 p 剪枝，并更新其他参数</li><li>Line 3：删除海森矩阵的 p 行 p 列，再求逆（这里用了数学的等价表达，降低了计算复杂度）</li></ul><h2 id="OBQ"><a href="#OBQ" class="headerlink" title="OBQ"></a>OBQ</h2><p>OBQ （和OBC是同一篇文章）指出，剪枝是一种特殊的量化（即剪枝的参数等价于量化到 0 点），我们只需要修改一下 OBC 的约束条件即可<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20250803211132.png" alt="image.png"><br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/v2-76796d5d98dc3cc27254d33199ea1ddc_1440w.png" alt="v2-76796d5d98dc3cc27254d33199ea1ddc_1440w.png"></p><h2 id="GPTQ"><a href="#GPTQ" class="headerlink" title="GPTQ"></a>GPTQ</h2><p>GPTQ 的创新点有：</p><ul><li>OBS 采用贪心策略，先量化对目标影响最小的参数；但 GPTQ 发现直接按顺序做参数量化，对精度影响也不大。</li><li>这项改进使得数矩阵每一行的量化可以做并行的矩阵计算，在大模型环境下，这项改进使得量化速度快了一个数量级；</li><li><strong>Lazy Batch-Updates</strong>，延迟一部分参数的更新，它能够缓解 bandwidth 的压力；</li><li><strong>Cholesky Reformulation</strong>，用 Cholesky 分解求海森矩阵的逆，在增强数值稳定性的同时，不再需要对海森矩阵做更新计算，进一步减少了计算量。<br>关注第二个创新点，也就是 Lazy Batch-Updates 是如何缓解 bandwidth 的压力的。<br><code>问题：</code>虽然 GPTQ 降低了时间复杂度，但这个算法的<strong>计算&#x2F;通信比太低</strong>，通信带宽成为了瓶颈。<br>例如在量化某个参数矩阵的情况下，每次量化一个参数，其他所有未量化的参数都要按公式全都要更新一遍：<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/v2-8cb25fd928d330bdaeef34af63b1da4f_1440w.png" alt="v2-8cb25fd928d330bdaeef34af63b1da4f_1440w.png"><br>如果每行的量化并行计算，那么每次更新过程就需要 read + write 一次参数矩阵。如果参数矩阵的维度为k \times k，那么量化这个参数矩阵就需要读写 k 次参数，总共的 IO 量为k^3个元素。当 k 比较大时（&gt;&#x3D; 4096），需要读写的元素就非常多了，运行时间大都被 IO 占据。<br><code>思路：</code>由于参数量化是一列一列按次序进行的，第 i 列的参数的量化结果受到前 i-1 列量化的影响，<strong>但第 i 列的量化结果不影响前面列的量化</strong>。因此我们不需要每次量化前面的列，就更新一遍第 i 列的参数，而是可以先记录第 i 列的更新量，在量化到第 i 列时，再一次性更新参数，这样就可以减少 IO 的次数。</li></ul><p><code>具体实现：</code>将参数矩阵按每 128 列划分为一个个 group，量化某一列时，group 内的参数立即更新，而 group 后面的列只记录更新量，延迟更新。当一个 group 的参数全部量化完成，再统一对后面的所有参数做一次更新。这就是 Lazy Batch-Updates。<br>Lazy Batch-Updates 不减少实际的计算量，但它能有效解决吞吐的瓶颈问题。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>GPTQ_MODEL实现方式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@torch.inference_mode()</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">quantize</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        blocksize=<span class="hljs-number">128</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-comment"># self.H = self.H.to(device=CUDA_0)</span><br>        <span class="hljs-comment"># log.info(f&quot;Quantization `&#123;self.name&#125;` using samples: `&#123;self.nsamples&#125;`&quot;)</span><br>        start = time.time()<br><br>        <span class="hljs-variable language_">self</span>.hessian_inverse = torch_compile(<span class="hljs-variable language_">self</span>.hessian_inverse)<br><br>        <span class="hljs-comment"># process buffered inputs</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.fwd_inputs_buffered_data) &gt; <span class="hljs-number">0</span>:<br>            torch_sync(device=<span class="hljs-variable language_">self</span>.module.target_device)<br><br>            <span class="hljs-keyword">for</span> inp <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.fwd_inputs_buffered_data:<br>                <span class="hljs-variable language_">self</span>.process_batch(inp)<br><br>            <span class="hljs-comment"># release buffer</span><br>            <span class="hljs-keyword">del</span> <span class="hljs-variable language_">self</span>.fwd_inputs_buffered_data<br><br>        <span class="hljs-comment"># if self.device.type not in [&quot;mps&quot;, &quot;cpu&quot;]:</span><br>        <span class="hljs-comment">#     self.module.weight.data = self.module.weight.data.cpu()</span><br><br>        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> waiting for pytorch implementation of ops for MPS</span><br>        <span class="hljs-keyword">if</span> sys.platform == <span class="hljs-string">&quot;darwin&quot;</span> <span class="hljs-keyword">and</span> os.getenv(<span class="hljs-string">&quot;PYTORCH_ENABLE_MPS_FALLBACK&quot;</span>) != <span class="hljs-string">&quot;1&quot;</span>:<br>            <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">&quot;For MacOS you must set env `PYTORCH_ENABLE_MPS_FALLBACK=1` before running quantization.&quot;</span>)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.module_copy <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># log.info(&quot;copy W to cuda_1&quot;)</span><br>            W = <span class="hljs-variable language_">self</span>._clone_module(device=<span class="hljs-variable language_">self</span>.module.target_device)<br>        <span class="hljs-keyword">else</span>:<br>            W = <span class="hljs-variable language_">self</span>.module_copy.to(device=<span class="hljs-variable language_">self</span>.module.target_device)<br>            <span class="hljs-keyword">del</span> <span class="hljs-variable language_">self</span>.module_copy<br><br>        <span class="hljs-variable language_">self</span>.quantizer.find_params(W, weight=<span class="hljs-literal">True</span>)<br><br>        H = <span class="hljs-variable language_">self</span>.H.to(device=<span class="hljs-variable language_">self</span>.module.target_device)<br>        <span class="hljs-keyword">del</span> <span class="hljs-variable language_">self</span>.H<br><br>        dead = torch.diag(H) == <span class="hljs-number">0</span><br>        H[dead, dead] = <span class="hljs-number">1</span><br>        W[:, dead] = <span class="hljs-number">0</span><br><br>        <span class="hljs-comment"># g_idx = []</span><br>        scale = []<br>        zero = []<br>        now_idx = <span class="hljs-number">1</span><br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.qcfg.static_groups:<br>            <span class="hljs-keyword">import</span> copy<br><br>            groups = []<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-variable language_">self</span>.columns, <span class="hljs-variable language_">self</span>.qcfg.group_size):<br>                quantizer = copy.deepcopy(<span class="hljs-variable language_">self</span>.quantizer)<br>                quantizer.find_params(W[:, i : (i + <span class="hljs-variable language_">self</span>.qcfg.group_size)], weight=<span class="hljs-literal">True</span>)<br><br>                scale.append(quantizer.scale)<br>                zero.append(quantizer.zero)<br>                groups.append(quantizer)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.qcfg.desc_act:<br>            perm = torch.argsort(torch.diag(H), descending=<span class="hljs-literal">True</span>)<br>            W = W[:, perm]<br>            H = H[perm][:, perm]<br>            invperm = torch.argsort(perm)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(<span class="hljs-variable language_">self</span>.qcfg, <span class="hljs-string">&quot;hyb_act&quot;</span>) <span class="hljs-keyword">and</span> <span class="hljs-variable language_">self</span>.qcfg.hyb_act <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> <span class="hljs-variable language_">self</span>.qcfg.desc_act:<br>            <span class="hljs-keyword">from</span> .gar <span class="hljs-keyword">import</span> compute_local_perms, compute_global_perm, compose_final_perm<br>            local_perms = compute_local_perms(torch.diag(H), <span class="hljs-variable language_">self</span>.qcfg.group_size)<br>            global_perm = compute_global_perm(torch.diag(H), <span class="hljs-variable language_">self</span>.qcfg.group_size)<br>            final_perm = compose_final_perm(local_perms, global_perm, <span class="hljs-variable language_">self</span>.qcfg.group_size)<br>            W = W[:, final_perm]<br>            H = H[final_perm][:, final_perm]<br><br>        Losses = torch.zeros_like(W)<br>        Q = torch.zeros_like(W)<br><br>        Hinv, damp = <span class="hljs-variable language_">self</span>.hessian_inverse(H)<br><br>        <span class="hljs-keyword">for</span> i1 <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-variable language_">self</span>.columns, blocksize):<br>            i2 = <span class="hljs-built_in">min</span>(i1 + blocksize, <span class="hljs-variable language_">self</span>.columns)<br>            count = i2 - i1<br><br>            W1 = W[:, i1:i2].clone()<br>            Q1 = torch.zeros_like(W1)<br>            Err1 = torch.zeros_like(W1)<br>            Losses1 = torch.zeros_like(W1)<br><br>            <span class="hljs-keyword">if</span> Hinv <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                Hinv1 = Hinv[i1:i2, i1:i2]<br><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(count):<br>                w = W1[:, i]<br>                <span class="hljs-keyword">if</span> Hinv <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    d = Hinv1[i, i]<br><br>                <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.qcfg.group_size != -<span class="hljs-number">1</span>:<br>                    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-variable language_">self</span>.qcfg.static_groups:<br>                        <span class="hljs-keyword">if</span> (i1 + i) % <span class="hljs-variable language_">self</span>.qcfg.group_size == <span class="hljs-number">0</span>:<br>                            <span class="hljs-variable language_">self</span>.quantizer.find_params(W[:, (i1 + i) : (i1 + i + <span class="hljs-variable language_">self</span>.qcfg.group_size)], weight=<span class="hljs-literal">True</span>)<br><br>                        <span class="hljs-keyword">if</span> ((i1 + i) // <span class="hljs-variable language_">self</span>.qcfg.group_size) - now_idx == -<span class="hljs-number">1</span>:<br>                            scale.append(<span class="hljs-variable language_">self</span>.quantizer.scale)<br>                            zero.append(<span class="hljs-variable language_">self</span>.quantizer.zero)<br>                            now_idx += <span class="hljs-number">1</span><br>                    <span class="hljs-keyword">else</span>:<br>                        idx = i1 + i<br>                        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.qcfg.desc_act:<br>                            idx = perm[idx]<br><br>                        <span class="hljs-variable language_">self</span>.quantizer = groups[idx // <span class="hljs-variable language_">self</span>.qcfg.group_size]<br><br>                q = <span class="hljs-variable language_">self</span>.quantizer.quantize(w.unsqueeze(<span class="hljs-number">1</span>)).flatten()<br>                Q1[:, i] = q<br>                <span class="hljs-keyword">if</span> Hinv <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    Losses1[:, i] = (w - q) ** <span class="hljs-number">2</span> / d**<span class="hljs-number">2</span><br>                    err1 = (w - q) / d<br>                    W1[:, i:] -= err1.unsqueeze(<span class="hljs-number">1</span>).matmul(Hinv1[i, i:].unsqueeze(<span class="hljs-number">0</span>))<br>                    Err1[:, i] = err1<br><br>            Q[:, i1:i2] = Q1<br>            <span class="hljs-keyword">if</span> Hinv <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                Losses[:, i1:i2] = Losses1 / <span class="hljs-number">2</span><br>                W[:, i2:] -= Err1.matmul(Hinv[i1:i2, i2:])<br><br>        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> why is there a torch_sync here? There are no streaming ops here?</span><br>        <span class="hljs-comment"># torch_sync(device=self.module.target_device)</span><br><br>        <span class="hljs-keyword">if</span> Hinv <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">del</span> Hinv<br>            <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.nsamples != <span class="hljs-number">0</span>:<br>                avg_loss = torch.<span class="hljs-built_in">sum</span>(Losses).item() / <span class="hljs-variable language_">self</span>.nsamples<br><br>                <span class="hljs-keyword">if</span> math.isnan(avg_loss):<br>                    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Losses sum item:&quot;</span>, torch.<span class="hljs-built_in">sum</span>(Losses).item())<br>                    <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f&quot;Quantization: Failed due to `NaN` loss for `<span class="hljs-subst">&#123;self.name&#125;</span>`&quot;</span>)<br>            <span class="hljs-keyword">else</span>:<br>                log.warn(<span class="hljs-string">f&quot;Quantization: `<span class="hljs-subst">&#123;self.name&#125;</span>` is not activated due to model inference logic (MoE)&quot;</span>)<br>                avg_loss = <span class="hljs-number">999999999</span><br>        <span class="hljs-keyword">else</span>:<br>            avg_loss = <span class="hljs-number">999999999</span><br><br>        <span class="hljs-keyword">del</span> Losses<br><br>        group_size = <span class="hljs-variable language_">self</span>.qcfg.group_size <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.qcfg.group_size != -<span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-variable language_">self</span>.columns<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.qcfg.static_groups <span class="hljs-keyword">and</span> <span class="hljs-variable language_">self</span>.qcfg.desc_act:<br>            g_idx = [perm[i] // group_size <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.columns)]<br>        <span class="hljs-keyword">else</span>:<br>            g_idx = [i // group_size <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.columns)]<br><br>        g_idx = torch.tensor(g_idx, dtype=torch.int32, device=Q.device)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.qcfg.desc_act:<br>            Q = Q[:, invperm]<br>            g_idx = g_idx[invperm]<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(<span class="hljs-variable language_">self</span>.qcfg, <span class="hljs-string">&quot;hyb_act&quot;</span>) <span class="hljs-keyword">and</span> <span class="hljs-variable language_">self</span>.qcfg.hyb_act <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> <span class="hljs-variable language_">self</span>.qcfg.desc_act:<br>            <span class="hljs-keyword">from</span> .gar <span class="hljs-keyword">import</span> invert_perm<br>            inv_final = invert_perm(final_perm)<br>            Q = Q[:, inv_final]<br>            inv_global_perm = invert_perm(global_perm)<br>            inv_global_perm_list = inv_global_perm.tolist()<br>            temp_scale = [ scale[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> inv_global_perm_list ]<br>            scale = temp_scale<br>            temp_zero = [ zero[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> inv_global_perm_list ]<br>            zero = temp_zero<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(<span class="hljs-variable language_">self</span>.module, transformers.Conv1D):<br>            Q = Q.t()<br><br>        <span class="hljs-keyword">if</span> Q.shape != <span class="hljs-variable language_">self</span>.module.weight.shape:<br>            Q = Q.reshape(<span class="hljs-variable language_">self</span>.module.weight.shape).type_as(<span class="hljs-variable language_">self</span>.module.weight.data)<br>        <span class="hljs-keyword">else</span>:<br>            Q = Q.type_as(<span class="hljs-variable language_">self</span>.module.weight.data)<br><br>        <span class="hljs-comment">#Q = Q.to(device=use_device)</span><br><br>        <span class="hljs-keyword">if</span> scale == []:<br>            scale.append(<span class="hljs-variable language_">self</span>.quantizer.scale)<br>            zero.append(<span class="hljs-variable language_">self</span>.quantizer.zero)<br><br>        scale = torch.cat(scale, dim=<span class="hljs-number">1</span>)<br>        zero = torch.cat(zero, dim=<span class="hljs-number">1</span>)<br><br>        duration = time.time() - start<br><br>        <span class="hljs-keyword">return</span> Q, scale, zero, g_idx, duration, avg_loss, damp, <span class="hljs-variable language_">self</span>.nsamples<br></code></pre></td></tr></table></figure><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">+-----------------------------+<br>|<span class="hljs-string"> 开始量化过程                </span>|<br>+-----------------------------+<br>               |<span class="hljs-string"></span><br><span class="hljs-string">               v</span><br><span class="hljs-string">+-----------------------------+</span><br><span class="hljs-string"></span>|<span class="hljs-string"> 初始化变量和参数            </span>|<br>|<span class="hljs-string"> - blocksize = 128           </span>|<br>|<span class="hljs-string"> - 记录开始时间              </span>|<br>+-----------------------------+<br>               |<span class="hljs-string"></span><br><span class="hljs-string">               v</span><br><span class="hljs-string">+-----------------------------+</span><br><span class="hljs-string"></span>|<span class="hljs-string"> 编译 Hessian 逆矩阵         </span>|<br>|<span class="hljs-string"> - self.hessian_inverse =    </span>|<br>|<span class="hljs-string">   torch_compile(self.hessian_inverse)</span><br><span class="hljs-string">+-----------------------------+</span><br><span class="hljs-string">               </span>|<br>               v<br>+-----------------------------+<br>|<span class="hljs-string"> 处理缓冲输入数据            </span>|<br>|<span class="hljs-string"> - 如果有缓冲数据，逐个处理   </span>|<br>|<span class="hljs-string"> - 清空缓冲数据              </span>|<br>+-----------------------------+<br>               |<span class="hljs-string"></span><br><span class="hljs-string">               v</span><br><span class="hljs-string">+-----------------------------+</span><br><span class="hljs-string"></span>|<span class="hljs-string"> 检查平台和环境变量          </span>|<br>|<span class="hljs-string"> - 如果是 MacOS 平台且未设置 </span>|<br>|<span class="hljs-string">   PYTORCH_ENABLE_MPS_FALLBACK </span>|<br>|<span class="hljs-string">   抛出错误                  </span>|<br>+-----------------------------+<br>               |<span class="hljs-string"></span><br><span class="hljs-string">               v</span><br><span class="hljs-string">+-----------------------------+</span><br><span class="hljs-string"></span>|<span class="hljs-string"> 复制权重矩阵 W              </span>|<br>|<span class="hljs-string"> - 如果 module_copy 为 None，</span>|<br>|<span class="hljs-string">   克隆模块到目标设备        </span>|<br>|<span class="hljs-string"> - 否则，使用 module_copy    </span>|<br>+-----------------------------+<br>               |<span class="hljs-string"></span><br><span class="hljs-string">               v</span><br><span class="hljs-string">+-----------------------------+</span><br><span class="hljs-string"></span>|<span class="hljs-string"> 初始化量化器参数            </span>|<br>|<span class="hljs-string"> - self.quantizer.find_params(W, weight=True)</span><br><span class="hljs-string">+-----------------------------+</span><br><span class="hljs-string">               </span>|<br>               v<br>+-----------------------------+<br>|<span class="hljs-string"> 处理 Hessian 矩阵           </span>|<br>|<span class="hljs-string"> - 将 Hessian 矩阵移动到目标设备 </span>|<br>|<span class="hljs-string"> - 删除原始 Hessian 矩阵      </span>|<br>|<span class="hljs-string"> - 处理 Hessian 矩阵的对角线 </span>|<br>+-----------------------------+<br>               |<span class="hljs-string"></span><br><span class="hljs-string">               v</span><br><span class="hljs-string">+-----------------------------+</span><br><span class="hljs-string"></span>|<span class="hljs-string"> 初始化变量                  </span>|<br>|<span class="hljs-string"> - scale, zero, now_idx      </span>|<br>+-----------------------------+<br>               |<span class="hljs-string"></span><br><span class="hljs-string">               v</span><br><span class="hljs-string">+-----------------------------+</span><br><span class="hljs-string"></span>|<span class="hljs-string"> 检查是否使用静态分组        </span>|<br>|<span class="hljs-string"> - 如果是，计算分组参数       </span>|<br>+-----------------------------+<br>               |<span class="hljs-string"></span><br><span class="hljs-string">               v</span><br><span class="hljs-string">+-----------------------------+</span><br><span class="hljs-string"></span>|<span class="hljs-string"> 检查是否使用描述性激活      </span>|<br>|<span class="hljs-string"> - 如果是，对权重矩阵和 Hessian 矩阵进行排序 </span>|<br>+-----------------------------+<br>               |<span class="hljs-string"></span><br><span class="hljs-string">               v</span><br><span class="hljs-string">+-----------------------------+</span><br><span class="hljs-string"></span>|<span class="hljs-string"> 检查是否使用混合激活        </span>|<br>|<span class="hljs-string"> - 如果是，计算局部和全局排列 </span>|<br>+-----------------------------+<br>               |<span class="hljs-string"></span><br><span class="hljs-string">               v</span><br><span class="hljs-string">+-----------------------------+</span><br><span class="hljs-string"></span>|<span class="hljs-string"> 初始化损失矩阵和量化矩阵    </span>|<br>|<span class="hljs-string"> - Losses 和 Q 初始化为零矩阵 </span>|<br>+-----------------------------+<br>               |<span class="hljs-string"></span><br><span class="hljs-string">               v</span><br><span class="hljs-string">+-----------------------------+</span><br><span class="hljs-string"></span>|<span class="hljs-string"> 计算 Hessian 逆矩阵         </span>|<br>|<span class="hljs-string"> - Hinv, damp = self.hessian_inverse(H)</span><br><span class="hljs-string">+-----------------------------+</span><br><span class="hljs-string">               </span>|<br>               v<br>+-----------------------------+<br>|<span class="hljs-string"> 按块处理权重矩阵            </span>|<br>|<span class="hljs-string"> - 遍历每一块，量化并更新权重 </span>|<br>+-----------------------------+<br>               |<span class="hljs-string"></span><br><span class="hljs-string">               v</span><br><span class="hljs-string">+-----------------------------+</span><br><span class="hljs-string"></span>|<span class="hljs-string"> 计算平均损失                </span>|<br>|<span class="hljs-string"> - 如果 Hinv 不为 None，计算平均损失 </span>|<br>|<span class="hljs-string"> - 否则，设置平均损失为极大值 </span>|<br>+-----------------------------+<br>               |<span class="hljs-string"></span><br><span class="hljs-string">               v</span><br><span class="hljs-string">+-----------------------------+</span><br><span class="hljs-string"></span>|<span class="hljs-string"> 清理资源                    </span>|<br>|<span class="hljs-string"> - 删除 Losses 矩阵           </span>|<br>+-----------------------------+<br>               |<span class="hljs-string"></span><br><span class="hljs-string">               v</span><br><span class="hljs-string">+-----------------------------+</span><br><span class="hljs-string"></span>|<span class="hljs-string"> 计算分组索引                </span>|<br>|<span class="hljs-string"> - 根据配置计算分组索引       </span>|<br>+-----------------------------+<br>               |<span class="hljs-string"></span><br><span class="hljs-string">               v</span><br><span class="hljs-string">+-----------------------------+</span><br><span class="hljs-string"></span>|<span class="hljs-string"> 处理权重矩阵的排列          </span>|<br>|<span class="hljs-string"> - 如果使用描述性激活或混合激活，反转排列 </span>|<br>+-----------------------------+<br>               |<span class="hljs-string"></span><br><span class="hljs-string">               v</span><br><span class="hljs-string">+-----------------------------+</span><br><span class="hljs-string"></span>|<span class="hljs-string"> 调整量化矩阵形状            </span>|<br>|<span class="hljs-string"> - 确保 Q 的形状与模块权重一致 </span>|<br>+-----------------------------+<br>               |<span class="hljs-string"></span><br><span class="hljs-string">               v</span><br><span class="hljs-string">+-----------------------------+</span><br><span class="hljs-string"></span>|<span class="hljs-string"> 返回量化结果                </span>|<br>|<span class="hljs-string"> - Q, scale, zero, g_idx, duration, avg_loss, damp, self.nsamples </span>|<br>+-----------------------------+<br></code></pre></td></tr></table></figure><p>参考链接：<br><a href="https://zhuanlan.zhihu.com/p/646210009">https://zhuanlan.zhihu.com/p/646210009</a><br><a href="https://zhuanlan.zhihu.com/p/680567656">https://zhuanlan.zhihu.com/p/680567656</a><br><a href="https://zhuanlan.zhihu.com/p/629517722">https://zhuanlan.zhihu.com/p/629517722</a><br><a href="https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=17c0a7de3c17d31f79589d245852b57d083d386e">https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=17c0a7de3c17d31f79589d245852b57d083d386e</a><br><a href="https://readpaper.feishu.cn/docx/OPP2dTuXAoaO0oxWhQAcC05Wnpc">https://readpaper.feishu.cn/docx/OPP2dTuXAoaO0oxWhQAcC05Wnpc</a><br><a href="https://github.com/ModelCloud/GPTQModel/blob/main/gptqmodel/quantization/gptq.py">https://github.com/ModelCloud/GPTQModel/blob/main/gptqmodel/quantization/gptq.py</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Recommender Systems with Generative Retrieval</title>
    <link href="/2024/11/04/Recommender-Systems-with-Generative-Retrieval/"/>
    <url>/2024/11/04/Recommender-Systems-with-Generative-Retrieval/</url>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/2305.05065">https://arxiv.org/pdf/2305.05065</a><br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20250804214111.png" alt="image.png"><br>现代推荐系统主要是通过在同一空间中构建查询emb和item的emb，然后通过大规模检索，在给定查询emb的情况下进行近似近邻搜索以选择最佳item。本文提出了一种新的生成检索方法Transformer Index for GEnerative Recommenders (TIGER) ，其中检索模型对目标item的标识符进行自回归解码。为此，作者构建了具有语义意义的码字（codeword）元组，作为每个item的语义ID。给定用户会话中item的语义ID，训练基于Transformer的seq-to-seq模型来预测用户将与之交互的下一个item的语义标识。<br>本文提出的TIGER主要分为两步：</p><ul><li>以内容特征生成语义ID：将item内容特征编码为emb向量，并将emb量化为语义码字的元组。由此产生的码字元组被称为item的语义ID</li><li>在语义ID上训练通用的推荐系统：构建transformer模型在语义id上训练用于序列推荐的模型</li></ul><h1 id="1-语义ID的生成"><a href="#1-语义ID的生成" class="headerlink" title="1 语义ID的生成"></a>1 语义ID的生成</h1><p>假设每个item都有相关的内容特征，这些特征捕捉有用的语义信息（例如标题或描述或图像）。采用预训练的内容编码器来生成语义emb。然后对语义emb进行量化，以生成每个item的语义ID。<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20250804220743.png" alt="image.png"><br>残差量化变分自动编码器（RQ-VAE）是一种多级向量量化器，在残差上进行量化来生成码字元组（语义ID）。通过更新量化码本和DNN编码器-解码器参数来联合训练自动编码器。为了防止RQ-VAE发生码本崩溃（大多数输入仅映射到少数码本向量），使用k均值聚类来初始化码本，将k-means算法应用于第一个训练批次（first training batch），并使用质心作为初始化。当然除了使用RQ-VAE，也可以使用其他的向量化方法，如LSH等。</p><h1 id="2-通过语义ID进行生成式retrival"><a href="#2-通过语义ID进行生成式retrival" class="headerlink" title="2 通过语义ID进行生成式retrival"></a>2 通过语义ID进行生成式retrival</h1><p><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20250804221416.png" alt="image.png"></p><p>推荐系统的“容错概率”要高于搜索，所以生成式召回可能会带来增益。</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>推荐</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>倒排索引</title>
    <link href="/2024/10/22/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95/"/>
    <url>/2024/10/22/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95/</url>
    
    <content type="html"><![CDATA[<h2 id="1-正向索引和反向索引"><a href="#1-正向索引和反向索引" class="headerlink" title="1 正向索引和反向索引"></a>1 正向索引和反向索引</h2><p>先介绍一下正向索引: 当用户发起查询时（假设查询为一个关键词），搜索引擎会扫描索引库中的所有文档，找出所有包含关键词的文档，这样依次从文档中去查找是否含有关键词的方法叫做正向索引。互联网上存在的网页（或称文档）不计其数，这样遍历的索引结构效率低下，无法满足用户需求。<br>正向索引结构如下:<br>文档1的ID→单词1的信息；单词2的信息；单词3的信息…<br>文档2的ID→单词3的信息；单词2的信息；单词4的信息…<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20241222204516.png" alt="image.png"><br>为了增加效率，<strong>搜索引擎会把正向索引变为反向索引（倒排索引）即把“文档→单词”的形式变为“单词→文档”的形式</strong>。倒排索引具体机构如下:<br>单词1→文档1的ID；文档2的ID；文档3的ID…<br>单词2→文档1的ID；文档4的ID；文档7的ID…<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20241222204541.png" alt="image.png"></p><h2 id="2-单词-文档矩阵"><a href="#2-单词-文档矩阵" class="headerlink" title="2 单词-文档矩阵"></a>2 单词-文档矩阵</h2><p>单词-文档矩阵是表达两者之间所具有的一种包含关系的概念模型。<br>现有以下几个文档:</p><pre><code class="hljs">D1：乔布斯去了中国。D2：苹果今年仍能占据大多数触摸屏产能。D3：苹果公司首席执行官史蒂夫·乔布斯宣布，iPad2将于3月11日在美国上市。D4：乔布斯推动了世界，iPhone、iPad、iPad2，一款一款接连不断。D5：乔布斯吃了一个苹果。</code></pre><p>此时用户查询为“苹果 And (乔布斯 Or iPad2)”，表示包含单词“苹果”，同时还包含“乔布斯”或“iPad2”的其中一个。</p><p>则“单词-文档”矩阵为:<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20241222204633.png" alt="image.png"><br>该矩阵可以从两个方向进行解读:</p><ul><li>纵向: 表示每个单独的文档包含了哪些单词，比如D1包含了“乔布斯这个词”，D4包含了“乔布斯”和“iPad2”。</li><li>横向: 表示哪些文档包含了该单词，比如D2、D3、D5包含了“苹果”这个词。<br>搜索引擎的索引其实就是实现“单词-文档”矩阵的具体数据结构。可以有不同的方式来实现上述概念模型，比如“倒排索引”、“签名文件”、“后缀树”等方式，但是“倒排索引”是实现单词到文档映射关系的最佳实现方式。</li></ul><h2 id="3-倒排索引"><a href="#3-倒排索引" class="headerlink" title="3 倒排索引"></a>3 倒排索引</h2><p>倒排索引(Inverted Index)：倒排索引是实现“单词-文档矩阵”的一种具体存储形式，通过倒排索引，可以根据单词快速获取包含这个单词的文档列表。倒排索引主要由两个部分组成：“单词词典”和“倒排文件”。</p><pre><code class="hljs">单词词典(Lexicon)：搜索引擎的通常索引单位是单词，单词词典是由文档集合中出现过的所有单词构成的字符串集合，单词词典内每条索引项记载单词本身的一些信息以及指向“倒排列表”的指针。倒排列表(PostingList)：倒排列表记载了出现过某个单词的所有文档的文档列表及单词在该文档中出现的位置信息，每条记录称为一个倒排项(Posting)。根据倒排列表，即可获知哪些文档包含某个单词。倒排文件(Inverted File)：所有单词的倒排列表往往顺序地存储在磁盘的某个文件里，这个文件即被称之为倒排文件，倒排文件是存储倒排索引的物理文件。</code></pre><p><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20241222204836.png" alt="image.png"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Multi-Task Learning</title>
    <link href="/2024/10/10/Multi-Task-Learning/"/>
    <url>/2024/10/10/Multi-Task-Learning/</url>
    
    <content type="html"><![CDATA[<p>多任务学习（Multi-task learning）是和单任务学习（single-task learning）相对的一种机器学习方法。在机器学习领域，标准的算法理论是一次学习一个任务，也就是系统的输出为实数的情况。复杂的学习问题先被分解成理论上独立的子问题，然后分别对每个子问题进行学习，最后通过对子问题学习结果的组合建立复杂问题的数学模型。多任务学习是一种联合学习，多个任务并行学习，结果相互影响。<br>多任务学习的核心思想是<strong>共享知识</strong>。不同任务之间可能存在某种关联或共享特征，而这些共同点可以通过共享模型参数来捕捉。</p><ol><li><p>硬共享：多个任务共享底层网络参数，只有顶层的任务特定网络是独立的。<br>它可以应用到所有任务的所有隐层上，而保留任务相关的输出层。硬共享机制降低了过拟合的风险。越多任务同时学习，我们的模型就能捕捉到越多任务的同一个表示，从而导致在我们原始任务上的过拟合风险越小。<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20241222191238.png" alt="image.png"></p></li><li><p>软共享：各任务有自己的模型，但通过正则化约束或参数融合来共享信息。<br>每个任务都由自己的模型，自己的参数。我们对模型参数的距离进行正则化来保障参数的相似。<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20241222191330.png" alt="image.png"></p></li></ol><p>多任务学习（Multitask Learning）是一种推导迁移学习方法，主任务（main tasks）使用相关任务（related tasks）的训练信号（training signal）所拥有的领域相关信息（domain-specific information），做为一直推导偏差（inductive bias）来提升主任务（main tasks）泛化效果（generalization performance）的一种机器学习方法。多任务学习涉及多个相关的任务同时并行学习，梯度同时反向传播，多个任务通过底层的共享表示（shared representation）来互相帮助学习，提升泛化效果。简单来说：多任务学习把多个相关的任务放在一起学习（注意，一定要是相关的任务，后面会给出相关任务（related tasks）的定义，以及他们共享了那些信息），学习过程（training）中通过一个在浅层的共享（shared representation）表示来互相分享、互相补充学习到的领域相关的信息（domain information），互相促进学习，提升泛化的效果。</p><p>为什么把多个相关的任务放在一起学习，可以提高学习的效果？关于这个问题，有很多解释。这里列出其中一部分，以图2中由单隐含层神经网络表示的单任务和多任务学习对比为例。</p><p>　　（1）多人相关任务放在一起学习，有相关的部分，但也有不相关的部分。当学习一个任务（Main task）时，与该任务不相关的部分，在学习过程中相当于是噪声，因此，引入噪声可以提高学习的泛化（generalization）效果。</p><p>　　（2）单任务学习时，梯度的反向传播倾向于陷入局部极小值。多任务学习中不同任务的局部极小值处于不同的位置，通过相互作用，可以帮助隐含层逃离局部极小值。</p><p>　　（3）添加的任务可以改变权值更新的动态特性，可能使网络更适合多任务学习。比如，多任务并行学习，提升了浅层共享层（shared representation）的学习速率，可能，较大的学习速率提升了学习效果。</p><p>　　（4）正则化机制。多任务学习通过引入归纳偏置（inductive bias）起到与正则化相同的作用。正是如此，它减小了模型过拟合的风险，同时降低了模型的Rademacher复杂度，即拟合随机噪音的能力。</p><p>还有很多潜在的解释，为什么多任务并行学习可以提升学习效果（performance）。多任务学习有效，是因为它是建立在多个相关的，具有共享表示（shared representation）的任务基础之上的，因此，需要定义一下，什么样的任务之间是相关的。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>QLoRA原理</title>
    <link href="/2024/09/30/QLoRA%E5%8E%9F%E7%90%86/"/>
    <url>/2024/09/30/QLoRA%E5%8E%9F%E7%90%86/</url>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/2305.14314">QLoRA</a>是一个使用量化思想对LoRA进行优化的量化算法，可以显著的降低训练大模型时所需要的显存资源。QLoRA的优化有三个核心要点：首先是定义了一种4位标准浮点数（Normal Float 4-bit，NF4）量化，基于分块的分位数量化的量化策略；其次是双重量化，包含对普通参数的一次量化和对量化常数的再一次量化，可以进一步减小缓存占用；最后是分页优化器（Page Optimizer），用来在显存过高时用一部分内存代替显存。</p><h2 id="1-背景知识"><a href="#1-背景知识" class="headerlink" title="1 背景知识"></a>1 背景知识</h2><p>模型量化和大模型的PEFT（Parameter-Efficient Fine-Tuning）有一个共同点是它们都希望模型计算能够更快。</p><h3 id="1-1-模型量化"><a href="#1-1-模型量化" class="headerlink" title="1.1 模型量化"></a>1.1 模型量化</h3><p>量化的本质是函数映射，根据量化过程是否线性我们可以把量化分为<strong>线性量化</strong>和<strong>非线性量化</strong>。量化过程是从一种数据类型“舍入”到另一种数据类型。举个例子，如果一种数据类型的范围为 <code>0..9</code>，而另一种数据类型的范围为 <code>0..4</code>，则第一种数据类型中的值 <code>4</code> 将舍入为第二种数据类型中的 <code>2</code> 。但是，如果在第一种数据类型中有值 <code>3</code>，它介于第二种数据类型的 <code>1</code> 和 <code>2</code> 之间，那么我们通常会四舍五入为 <code>2</code>。也就是说，第一种数据类型的值 <code>4</code> 和 <code>3</code> 在第二种数据类型中具有相同的值 <code>2</code>。这充分表明量化是一个有噪过程，会导致信息丢失，是一种有损压缩。</p><h3 id="1-2-分位数量化"><a href="#1-2-分位数量化" class="headerlink" title="1.2 分位数量化"></a>1.2 分位数量化</h3><p>一种基于数据分布的量化方法，通过将数据按其分布的分位点划分成若干区间，并将每个区间的值映射为一个固定的量化值。这种方法特别适用于处理非均匀分布的数据，可以显著提高量化的表示能力和模型性能。<strong>分位数</strong>（Quantile）在数学上的定义指的是把<strong>顺序排列</strong>的一组数据分割为若干个相等块的分割点的数值。</p><h3 id="1-3-分块k位量化"><a href="#1-3-分块k位量化" class="headerlink" title="1.3 分块k位量化"></a>1.3 分块k位量化</h3><p><strong>分块 k-位量化（Blockwise kkk-bit Quantization）</strong> 是一种针对大规模深度学习模型优化的量化方法，通过将模型参数或激活值划分为多个小块，对每个块分别进行 kkk-位量化，从而在保持模型性能的同时大幅减少内存占用和计算复杂度。</p><h3 id="1-4-LoRA"><a href="#1-4-LoRA" class="headerlink" title="1.4 LoRA"></a>1.4 LoRA</h3><p>LoRA是一个基于适配器学习的PEFT算法。它指出模型往往是过参数化话，因此可以用两个低秩矩阵代替原来的密集连接，从而可以减少模型的参数量。另外LoRA的适配器是一个和原模型的网络块并行的结构，在推理时计算的是已经将适配器的参数加到原模型参数上的新参数，因此不会带来任何的推理时间的增加。</p><h2 id="2-QLoRA"><a href="#2-QLoRA" class="headerlink" title="2 QLoRA"></a>2 QLoRA</h2><p>QLoRA的工作有三个，第一个工作是结合了分位数量化和分块量化的<strong>4位标准浮点数量化</strong>（4-bit NormalFloat Quantization）。第二个工作是对模型进行两次量化的<strong>双重量化</strong>（Double Quantization），它的第二次量化只作用在第一次量化产生的量化常数上，可以进一步节约显存占用。第三个工作是<strong>分页优化</strong>（Paged Optimizer），使用CPU内存代替GPU显存保存部分梯度参数。</p><p>使用上面介绍的分位数量化方法我们可以将FP2精度量化到4bit的精度，但是直接这么用的一个问题是不能保证高精度的0一定被映射到低精度的0。为了确保零点映射到0并且使用4位数据类型的全部16位，我们通过估计正负两个范围的分位数来创建一个非对称的数据类型：负数部分映射其中7位，正数部分映射8位，0占据1位，总共用满了4位数的16位。另外我们也可以使用对称的量化，其中正数和负数均使用7位，0占用2个位。我这里和论文介绍的略有不同，论文说的是正数部分取9个值，负数部分取8个值，不过它们都会取到0，所以合并时再去掉一个重复的0，这两个说法其实是一样的，只是实现方式略有差异。</p><p>当我们保存模型时我们不仅要保存量化后的结果，还要保存每个块的量化常数。虽然量化后的参数只有4bit的精度，但是这个量化常量的精度是float32。在QLoRA中，每个块的大小是64，因为块中的每个值占4比特。这相当于为了存储量化常数，模型要额外占用显存。QLoRA的双重量化就是对这个量化常数再做一次8bit的量化，在进行量化常数的量化时，QLoRA以每256个量化常数为一组再做一次量化。因此它额外增加的内存消耗有两部分组成，一部分是量化后的8bit的第一层的量化常数，第二部分是为量化常数做量化的第二层的32bit的量化常数。因为使用了双重量化，在进行反量化时我们也需要进行两次反量化才能把量化后的值还原。</p><p>分页优化是针对梯度检查点做的进一步优化，以防止在显存使用峰值时发生显存OOM的问题。QLoRA分页优化其实就是当显存不足是，将保存的部分梯度检查点转移到CPU内存上，和计算机的内存数据转移到硬盘上的常规内存分页一个道理。</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>#PEFT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LoRA原理</title>
    <link href="/2024/09/22/LoRA%E5%8E%9F%E7%90%86/"/>
    <url>/2024/09/22/LoRA%E5%8E%9F%E7%90%86/</url>
    
    <content type="html"><![CDATA[<p>LORA 是一种低资源微调大模型方法，出自论文 <a href="https://arxiv.org/pdf/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a>。 使用 LORA，训练参数仅为整体参数的万分之一、GPU 显存使用量减少 2&#x2F;3 且不会引入额外的推理耗时。</p><h2 id="高效微调（PEFT）的基本原理"><a href="#高效微调（PEFT）的基本原理" class="headerlink" title="高效微调（PEFT）的基本原理"></a>高效微调（PEFT）的基本原理</h2><p>full fine-tuning：在微调过程中模型加载预训练参数进行初始化，并通过最大化语言模型概率进行参数更新，这种微调方式的主要缺点是我们学习到的参数增量的维度和预训练参数是一致的，需要的资源较多。<br>高效微调方法则通过选择性地调整少量参数或增加额外的小型模块来显著降低训练成本，同时保持良好的模型性能。</p><h2 id="LoRA实现方式"><a href="#LoRA实现方式" class="headerlink" title="LoRA实现方式"></a>LoRA实现方式</h2><h3 id="Instrisic-Dimension"><a href="#Instrisic-Dimension" class="headerlink" title="Instrisic Dimension"></a>Instrisic Dimension</h3><ol><li>为何用数千的样本就能将一个数十亿参数的模型微调得比较好？</li><li>为何大模型表现出很好的 few-shot 能力？<br><a href="https://arxiv.org/abs/2012.13255?utm_source=chatgpt.com">https://arxiv.org/abs/2012.13255?utm_source=chatgpt.com</a><br>Aghajanyan 的研究表明：预训练模型拥有极小的内在维度 (instrisic dimension)，即存在一个极低维度的参数，微调它和在全参数空间中微调能起到相同的效果。同时 Aghajanyan 发现在预训练后，越大的模型有越小的内在维度，这也解释了为何大模型都拥有很好的 few-shot 能力。</li></ol><h3 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h3><p><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20241222160523.png" alt="image.png"><br>LoRA（Low-Rank Adaptation of Large Language Models） 的实现核心是通过 低秩矩阵分解 来对模型权重进行高效微调，而不需要更新模型的全部参数。<br>LoRA 假设模型的权重矩阵在微调过程中只需要少量的调整。<br>因此，可以将模型权重表示为：<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20241222160708.png" alt="image.png"><br><strong>在训练过程中，低秩的适应矩阵</strong>ΔW<strong>仅仅放大了对下游任务有用的特征，而不是预训练模型中的主要特征。</strong><br>在初始化时，A使用高斯初始化，B使用零矩阵进行的初始化。因为 通常是一个非常小的值（实验证明1，2，4，8的效果就非常好），所以LoRA在训练时引入的参数量是非常小的，因此它的训练也是非常高效的，也不会带来显著的显存增加。<br>LoRA要求 或者 其中之一必须使用零矩阵进行初始化，这样当数据第一次通过网络时，它和预训练的结果是一致的，这样便保证了模型在初始阶段便有一个不错的效果。苏剑林老师指出，这种一个全零，一个非全零的方式带来了不对称的问题<a href="https://zhuanlan.zhihu.com/p/663557294#ref_3">[3]</a>，其实我们也可以使用两个非全零矩阵进行初始化，但是需要事先将预训练权重减去初始化的值。</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>PEFT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PEFT概述</title>
    <link href="/2024/09/17/PEFT%E6%A6%82%E8%BF%B0/"/>
    <url>/2024/09/17/PEFT%E6%A6%82%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<p><strong>PEFT（Parameter-Efficient Fine-Tuning）</strong> 是一种针对大规模预训练模型（如 GPT、BERT 等）设计的高效微调技术框架。PEFT 的目标是在不更新或仅少量更新模型参数的情况下，使模型能够高效适配新任务。这种方法大幅降低了微调的计算和存储成本，并在多任务学习和领域适配中展现了强大的灵活性。</p><h2 id="1-LoRA（Low-Rank-Adaptation）"><a href="#1-LoRA（Low-Rank-Adaptation）" class="headerlink" title="1 LoRA（Low-Rank Adaptation）"></a>1 LoRA（Low-Rank Adaptation）</h2><p>对模型的权重调整部分使用低秩分解，只优化新增的低秩矩阵</p><h2 id="2-Prefix-Tuning"><a href="#2-Prefix-Tuning" class="headerlink" title="2 Prefix Tuning"></a>2 Prefix Tuning</h2><p>在输入序列前添加一段可训练的“前缀”，这些前缀参数独立于模型原始参数。<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20241222184025.png" alt="image.png"></p><h2 id="3-Adapter-Tuning"><a href="#3-Adapter-Tuning" class="headerlink" title="3 Adapter Tuning"></a>3 Adapter Tuning</h2><p>在预训练模型的中间层中插入小的可训练层或“适配器”。这些适配器通常包括一些全连接层、非线性激活函数等，它们被设计用来捕获特定任务的知识，而不需要对整个预训练模型进行大规模的微调。<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20241222184114.png" alt="image.png"></p><h2 id="4-Prompt-engineering"><a href="#4-Prompt-engineering" class="headerlink" title="4 Prompt engineering"></a>4 Prompt engineering</h2><p>优化输入中的提示（Prompt）部分，而非模型本身的参数。</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>PEFT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RAG项目总结V1.0</title>
    <link href="/2024/08/30/RAG%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93V1.0/"/>
    <url>/2024/08/30/RAG%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93V1.0/</url>
    
    <content type="html"><![CDATA[<h2 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a>项目地址</h2><p><strong>GitHub地址：</strong><br><a href="https://github.com/Chunfei-He/MyRAG">Chunfei-He&#x2F;MyRAG (github.com)</a></p><h2 id="项目进度"><a href="#项目进度" class="headerlink" title="项目进度"></a>项目进度</h2><ol><li><p>数据处理pipeline done</p></li><li><p>目前数据源：项目README.md文件、阿里2023财报.pdf、斗破苍穹小说.txt、iPhone使用手册（ios5）.pdf、中华人民共和国消费者权益保护法.pdf</p></li><li><p>实现功能：搭建web demo页面可针对上述文件提问，对于没有相关信息的问题可以拒绝回复。<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20240830130822.png" alt="image.png"></p></li><li><p>存在问题：</p><ol><li>iPhone使用手册版本较旧，缺少iPad使用手册。未考虑同一问题（如：如何拍照）可能来自两种设备持有者的情况</li><li>阿里财报为繁体，导致回复时出现部分繁体字<br> <img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20240830130632.png" alt="image.png"></li></ol></li><li><p>后续升级技术方案：</p><ol><li>针对问题一，计划在向量相似度计算之前加入router组件，先判断需要调用哪个向量库，再进行针对性调用</li><li>针对问题二，修改prompt模板，强调使用简体中文回复</li><li>待实现功能有：定义金融指标，利用财报数据计算。计划加到router组件中，判断是否需要使用数据查询功能（直接从表格中查询数据，而不是计算相似度）</li><li>可升级方向有：<ol><li>目前只从数据库中召回相似度排名top 1的数据作为上下文，后续可以优化召回内容，比如召回多条、多次问答再总结</li><li>对于markdown这种结构化语言，包括使用手册这种每一块有明确主题的文档，可以使用标题作为检索key</li><li>可以考虑加入召回后排序</li></ol></li></ol></li></ol><h2 id="项目架构"><a href="#项目架构" class="headerlink" title="项目架构"></a>项目架构</h2><p>component是RAG的组件，分为四大部分（数据切分，向量化，向量存储，大模型）<br>data用于存放需要嵌入的文件（兼容Pdf  TXT，md文件）<br>db用于存放向量化后的数据，也是数据库的加载路径<br>build.ipynb构建向量数据库<br>webdemo_by_gradio使用gradio基于嵌入的文件调用OpenAI的回答助手</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs markdown">MyRAG<br>├─ build.ipynb<br>├─ component<br>│  ├─ databases.py<br>│  ├─ data<span class="hljs-emphasis">_chunker.py</span><br><span class="hljs-emphasis">│  ├─ embedding.py</span><br><span class="hljs-emphasis">│  └─ llms.py</span><br><span class="hljs-emphasis">├─ data</span><br><span class="hljs-emphasis">│  ├─ dpcq.txt</span><br><span class="hljs-emphasis">│  ├─ README.md</span><br><span class="hljs-emphasis">│  ├─ 中华人民共和国消费者权益保护法.pdf</span><br><span class="hljs-emphasis">│  ├─ iphone 5_</span>user<span class="hljs-emphasis">_guide_</span>ch.pdf<br>│  └─ alibaba.pdf<br>├─ db<br>│  ├─ doecment.json<br>│  └─ vectors.json<br>├─ README.md<br>└─ webdemo<span class="hljs-emphasis">_by_</span>gradio.ipynb<br><br></code></pre></td></tr></table></figure><h2 id="什么是RAG？"><a href="#什么是RAG？" class="headerlink" title="什么是RAG？"></a>什么是RAG？</h2><p>检索增强生成(RAG) 是<strong>一种使用来自私有或专有数据源的信息来辅助文本生成的技术</strong>。<br>它将检索模型（设计用于搜索大型数据集或知识库）和生成模型（例如大型语言模型(LLM)，此类模型会使用检索到的信息生成可供阅读的文本回复）结合在一起。 </p><h2 id="为什么需要RAG？"><a href="#为什么需要RAG？" class="headerlink" title="为什么需要RAG？"></a>为什么需要RAG？</h2><p>LLM会产生误导性的 “幻觉”，依赖的信息可能过时，处理特定知识时效率不高，缺乏专业领域的深度洞察，同时在推理能力上也有所欠缺。<br>正是在这样的背景下，检索增强生成技术（Retrieval-Augmented Generation，RAG）应时而生，成为 AI 时代的一大趋势。<br>RAG 通过在语言模型生成答案之前，先从广泛的文档数据库中检索相关信息，然后利用这些信息来引导生成过程，极大地提升了内容的准确性和相关性。RAG 有效地缓解了幻觉问题，提高了知识更新的速度，并增强了内容生成的可追溯性，使得大型语言模型在实际应用中变得更加实用和可信。</p><p>RAG的基本结构有哪些呢？</p><ul><li>要有一个向量化模块，用来将文档片段向量化。</li><li>要有一个文档加载和切分的模块，用来加载文档并切分成文档片段。</li><li>要有一个数据库来存放文档片段和对应的向量表示。</li><li>要有一个检索模块，用来根据 Query （问题）检索相关的文档片段。</li><li>要有一个大模型模块，用来根据检索出来的文档回答用户的问题。</li></ul><p>参考下图，RAG的基本流程有：</p><ul><li><strong>索引</strong>：将文档库分割成较短的 Chunk，并通过编码器构建向量索引。</li><li><strong>检索</strong>：根据问题和 chunks 的相似度检索相关文档片段。</li><li><strong>生成</strong>：以检索到的上下文为条件，生成问题的回答。<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20240901174958.png" alt="image.png"></li></ul><h3 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h3><p>向量化的类主要是用来将文档片段向量化，将一段文本映射为一个向量。</p><p><code>get_embedding</code>方法：获取文本的向量表示<br><code>cosine_similarity</code>方法：计算两个向量之间的余弦相似度</p><h3 id="文档加载和切分"><a href="#文档加载和切分" class="headerlink" title="文档加载和切分"></a>文档加载和切分</h3><p>类主要是用来加载文档并切分成文档片段。这个文档可以是一篇文章，一本书，一段对话，一段代码等等。这个文档的内容可以是任何的，只要是文本就行。比如：pdf文件、md文件、txt文件等等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_file_content</span>(<span class="hljs-params">cls, file_path: <span class="hljs-built_in">str</span></span>):<br>    <span class="hljs-comment"># 根据文件扩展名选择读取方法</span><br>    <span class="hljs-keyword">if</span> file_path.endswith(<span class="hljs-string">&#x27;.pdf&#x27;</span>):<br>        <span class="hljs-keyword">return</span> cls.read_pdf(file_path)<br>    <span class="hljs-keyword">elif</span> file_path.endswith(<span class="hljs-string">&#x27;.md&#x27;</span>):<br>        <span class="hljs-keyword">return</span> cls.read_markdown(file_path)<br>    <span class="hljs-keyword">elif</span> file_path.endswith(<span class="hljs-string">&#x27;.txt&#x27;</span>):<br>        <span class="hljs-keyword">return</span> cls.read_text(file_path)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;Unsupported file type&quot;</span>)<br></code></pre></td></tr></table></figure><p>按 Token 的长度来切分文档，可以设置一个最大的 Token 长度，然后根据这个最大的 Token 长度来切分文档。这样切分出来的文档片段就是一个一个的差不多相同长度的文档片段了。<br>在切分的时候要注意，片段与片段之间最好要有一些重叠的内容，这样才能保证检索的时候能够检索到相关的文档片段。还有就是切分文档的时候最好以句子为单位，也就是按 <code>\n</code> 进行粗切分，这样可以基本保证句子内容是完整的。</p><h3 id="数据库-向量检索"><a href="#数据库-向量检索" class="headerlink" title="数据库&amp;&amp;向量检索"></a>数据库&amp;&amp;向量检索</h3><p>设计一个向量数据库用来存放文档片段和对应的向量表示了。<br>设计一个检索模块，用来根据 Query （问题）检索相关的文档片段。</p><ul><li><code>persist</code>：数据库持久化，本地保存</li><li><code>load_vector</code>：从本地加载数据库</li><li><code>get_vector</code>：获得文档的向量表示</li><li><code>query</code>：根据问题检索相关的文档片段<br><code>query</code> 方法具体实现：<br>首先先把用户提出的问题向量化，然后去数据库中检索相关的文档片段，最后返回检索到的文档片段。</li></ul><h3 id="大模型"><a href="#大模型" class="headerlink" title="大模型"></a>大模型</h3><p>根据检索出来的文档回答用户的问题。</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>智能生成</title>
    <link href="/2024/08/12/%E6%99%BA%E8%83%BD%E7%94%9F%E6%88%90/"/>
    <url>/2024/08/12/%E6%99%BA%E8%83%BD%E7%94%9F%E6%88%90/</url>
    
    <content type="html"><![CDATA[<p>除了基于常规的文档做问答，实际上还可以基于数据类做问答：</p><ul><li><p><strong>基于Mysql数据库：</strong>通过Embedding的方式将表的Schema、表数据全部向量化，然后用户可以问问题比如“查询王五的考试成绩”，<strong>模型根据上下文自动生成SQL查询语句，然后调用工具执行</strong></p></li><li><p>基于数据仓库：同样是通过Embedding的方式将表的Schema、表数据全部向量化，然后用户可以问问题比如“查询某某团队的代码提交数据”，<strong>模型根据上下文自动生成查询策略，然后调用工具执行</strong></p></li></ul><p><strong>这里会引入一个新的问题：什么时候需要Embedding + LLM？什么时候只需要LLM？</strong></p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于embeddings+GPT的Q&amp;A探索</title>
    <link href="/2024/08/10/%E5%9F%BA%E4%BA%8Eembeddings-GPT%E7%9A%84Q&amp;A%E6%8E%A2%E7%B4%A2/"/>
    <url>/2024/08/10/%E5%9F%BA%E4%BA%8Eembeddings-GPT%E7%9A%84Q&amp;A%E6%8E%A2%E7%B4%A2/</url>
    
    <content type="html"><![CDATA[<p><a href="https://liaoxuefeng.com/blogs/all/2023-07-31-qa-using-embeddings/index.html">https://liaoxuefeng.com/blogs/all/2023-07-31-qa-using-embeddings/index.html</a></p><h1 id="1、当前存在的问题"><a href="#1、当前存在的问题" class="headerlink" title="1、当前存在的问题"></a>1、当前存在的问题</h1><p>我们希望把API文档、帮助文档等“事实回忆”的内容，通过Chat的方式可以快速获得。当前的GPT、一言等存在以下几个问题：</p><ul><li>【时效性】训练数据是基于某个时间之前的数据，缺少最新的数据，而且每一次增加最新的数据都会带来高昂的训练成本</li><li>【安全性】无法访问企业内部私密的文档，且这些文档是不能用来Fine-Tunning的</li><li>【Token限制】每次会话会有Token限制，而文档内容是比较多的，使用Prompt的方式会远超Token限制</li></ul><h6 id="事实回忆是指什么？"><a href="#事实回忆是指什么？" class="headerlink" title="事实回忆是指什么？"></a>事实回忆是指什么？</h6><p>可以理解为通过Chat的方式做检索。举个例子来说，如果手里有大批的iPipe、iCafe、iCode帮助文档和问答语料，我们希望用户在获取相关功能的内容介绍时、编译失败查看为什么失败时、代码合入冲突如何解决时、查看某个产品有哪些OpenAPI时等等，这些都算是事实回忆。</p><h1 id="2、可行方案"><a href="#2、可行方案" class="headerlink" title="2、可行方案"></a>2、可行方案</h1><h2 id="2-1、什么是Embeddings"><a href="#2-1、什么是Embeddings" class="headerlink" title="2.1、什么是Embeddings"></a>2.1、什么是Embeddings</h2><p>重点了解Embeddings是做什么、怎么用，对其内部原理不做过多分析。</p><p><strong>Embedding</strong> 是一种以向量的方式表示“数据”的策略，这里的“数据”可以是一个物体、一个词语、一段对话、一部电影等等。无需完全理解其原理，可以简单的记为Embeddings是通过向量的方式来表示真实数据，它可以将简单的数据升维，让数据特征更明显，也可以让复杂的数据降维，让它更容易理解和计算。同时Embeddings还有“相加”、“相减”的能力，这些能力都是通过他的本质数据结构——向量来完成的。</p><ul><li><strong>升维：升维的目的是将低维数据一些特征放大，便于提取</strong></li><li><strong>降维：降维的目的让复杂的数据更容易理解和计算，并且减少无效空间的占用_（比如一篇文章1w字，其中1000个字是“的”，那么可以通过降维的手段来把它压缩）</strong></li><li><strong>相加和相减</strong></li><li><strong>相似性计算</strong>：相似性比较有很多算法，比如比较简单<a href="https://en.wikipedia.org/wiki/Cosine_similarity">余弦相似度</a>，本质也是在通过向量计算。</li></ul><h2 id="2-2、详细方案"><a href="#2-2、详细方案" class="headerlink" title="2.2、详细方案"></a>2.2、详细方案</h2><p>之所以用大段文字描述Embeddings，是因为我们需要使用Embeddings方式来压缩大文本数据，同时做检索，从而解决Prompt Token过长的问题。<br>方案分为<strong>准备Embeddings数据、检索Embeddings数据、发起Query获得响应</strong>三部分，具体如下：</p><ul><li><strong>准备Embeddings数据</strong><ul><li><strong>收集：</strong> 将帮助文档、API文档、用户问答语料统一收集，或者需要其他知识文档也可以收集起来</li><li><strong>切片：将文档切片，因为收集的文档普遍较大，不建议将整个文档转换为Embeddings，而是切分成若干段落，这样在后续的检索、组装Prompt也更有效</strong></li><li><strong>生成：通过调用OpenAI的接口将各个切片生成Embeddings</strong></li><li><strong>存储：将生成的Embeddings存储，可以使用向量数据库</strong></li></ul></li></ul><p>这一步其实是在构建一个检索库，也可以把<strong>Embeddings</strong>当成索引，向量数据库想象成ES。</p><ul><li><strong>检索Embeddings数据</strong><ul><li>用户提问获得Query，调用OpenAI的API获取Query的Embeddings</li><li>使用这个Embeddings去向量数据库中做相似性计算，获取权重最高的n个切片</li></ul></li><li><strong>发起Query获得响应</strong><ul><li>将n个切片和用户原始Query组装成Prompt，注意Prompt不能超出Token限制，如果超出限制要多n个切片进行低权重淘汰</li><li>发送信息获取响应<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/Pasted%20image%2020240814171036.png" alt="Pasted image 20240814171036.png"></li></ul></li></ul><p>一种基于搜索-提问的两步法，使GPT能够使用参考文本库回答问题。</p><ol><li>搜索：在文本库中搜索相关文本片段；</li><li>提问：将检索到的文本片段连同消息一并发送至GPT并提问问题。<br>文本搜索相对于微调的一个缺点是每个模型都受到一次可以读取的最大文本量的限制。</li></ol><h2 id="2-3、Embeddings的缺陷"><a href="#2-3、Embeddings的缺陷" class="headerlink" title="2.3、Embeddings的缺陷"></a>2.3、Embeddings的缺陷</h2><ul><li>多次调用模型API生成Embeddings，会耗费大量的资源（Money）</li><li>对于切片的粒度要求很高，不能太粗或太细，同时高度依赖于Embeddings的生成质量</li></ul><h1 id="3、AutoGPT的方案"><a href="#3、AutoGPT的方案" class="headerlink" title="3、AutoGPT的方案"></a>3、AutoGPT的方案</h1><p>最近大火的AutoGPT实现方案和以上内容也有相似之处，其实并不是一个算法、策略的创新，而是一种工程、产品的创新，为了方便理解，做简要分析对比：<em>理论上说，ChatPDF、AgentGPT等各种工具都是通过类似的方式完成。</em></p><ul><li>用户输入ai_name（角色）、ai_role（角色描述）、ai_goal（5个目标）</li><li>让ChatGPT根据输入的角色和目标，分步生成操作序列</li><li>根据操作序列，调用不同的命令组件（如Shell脚本、网页爬虫、语音播报、Google搜索等）</li><li>组件调用后的输出存储到缓存或长期记忆中，作为下一个组件的输入</li><li>直到最后一个组件执行完成，任务结束<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/Pasted%20image%2020240814171805.png" alt="Pasted image 20240814171805.png"></li></ul><ol><li><a href="https://github.com/openai/openai-cookbook/blob/d67c4181abe9dfd871d382930bb778b7014edc66/examples/Embedding_long_inputs.ipynb">https://github.com/openai/openai-cookbook/blob/d67c4181abe9dfd871d382930bb778b7014edc66/examples/Embedding_long_inputs.ipynb</a></li><li>﻿<a href="https://github.com/openai/openai-cookbook/blob/d67c4181abe9dfd871d382930bb778b7014edc66/examples/Embedding_Wikipedia_articles_for_search.ipynb">https://github.com/openai/openai-cookbook/blob/d67c4181abe9dfd871d382930bb778b7014edc66/examples/Embedding_Wikipedia_articles_for_search.ipynb</a>﻿</li><li>﻿<a href="https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb">https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb</a>﻿</li><li>﻿<a href="https://simonwillison.net/2023/Jan/13/semantic-search-answers/">https://simonwillison.net/2023/Jan/13/semantic-search-answers/</a></li></ol><h3 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h3><h4 id="知识库原始数据向量化的过程中"><a href="#知识库原始数据向量化的过程中" class="headerlink" title="知识库原始数据向量化的过程中"></a>知识库原始数据向量化的过程中</h4><ul><li>如何选择合适的向量库<br>  选择向量库，主要可以通过以下几个方面考虑：<ul><li>向量库本身的可扩展性和可靠性，以及引入向量库怎么保持多个系统之间的一致性</li><li>是否能在相似度搜索基础上支持 metadata 条件查询</li><li>是否支持多种类型的用户 query，例如关键词搜索，文档总结，多个文档之间的对比等</li><li>是否能支持复杂的 index 结构，例如有层级关系的 index</li></ul></li><li>如何进行巧妙的切分（chunks）<br>  对于帮助文档存在API文档、用户手册（包含大量图片、视频）、FAQ等类型，不同的文档内容结构是不一样的，因此选择通用的切分方案会将连贯的数据进行切散，如下是一个典型的MD文档，如果复用MD通用拆分方法，直接使用“#”关键词做切割，那么会切割成5部分，这种切法已经把文档一个连贯的描述切散<ul><li><strong>方案一：直接使用chunk size作为切割条件</strong><ul><li>完整的描述被切碎</li><li>用户的问题和切碎后的文本做相似性匹配时召回率非常低</li></ul></li><li><strong>方案二：基于MD的标准格式进行切分</strong><ul><li>保证了每个文本块的连续性描述</li><li>用户的问题往往很短且形式多样，但完整的文本块描述比较长，仍然会导致做相似性匹配时召回率非常低</li></ul></li><li><strong>方案三：基于MD的标准格式进行切分，但只向量化标题</strong><ul><li>保证了每个文本块的连续性描述</li><li>用户的问题往往很短且形式多样，做相似性匹配时只匹配标题，提高召回率</li></ul></li><li><strong>方案四：在方案三的基础上，新增关键词提取向量化过程</strong><ul><li>使用LLM针对答案进行关键词提取，约定关键词提取数量，比如最多提取5个</li><li>提取关键词之后直接使用原始文本存入向量库中</li><li>用户提问时一方面将Query向量化，另一方面实用LLM对Query进行关键词提取，即使用Query向量化 + Query关键词两个条件去向量库做相似性匹配，从而再次提高召回质量</li></ul></li></ul></li></ul><p>几篇文档切分论文：</p><ul><li>“<a href="https://www.mdpi.com/2078-2489/13/2/83#">A Survey on Text Segmentation Algorithms</a>“ - 这篇论文概述了文本分割算法的不同方法和技术，包括基于规则和统计的方法、深度学习方法等，对于了解该领域的研究现状和发展趋势有帮助。</li><li>“Semantic Text Segmentation using Deep Learning” - 这篇论文提出了一种基于深度学习的语义文本分割方法，使用卷积神经网络（CNN）和长短时记忆网络（LSTM）对文本进行建模，实现了较好的分割效果。</li><li>“Unsupervised Text Segmentation based on Semantic Similarity and Word Embedding” - 这篇论文提出了一种无监督的文本分割方法，利用语义相似度和词嵌入技术对文本进行分割，对于处理无标签的文本数据有一定的借鉴意义。</li></ul><h4 id="用户回答过程中"><a href="#用户回答过程中" class="headerlink" title="用户回答过程中"></a>用户回答过程中</h4><ul><li>如何提高召回的质量<br>方案三中已经提到了可以通过只对标题向量化提高召回质量，实际上这里做了一个预设，预设用户提的问题都是和标题含义差不多的。但答案里边实际上会有更多的内容可以回答更多的问题，因此，我们还可以基于LLM的能力反向生成一些列问题，并向量化</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>RAG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计划和解决提示：通过大型语言模型改进零样本思维链推理</title>
    <link href="/2024/08/06/%E8%AE%A1%E5%88%92%E5%92%8C%E8%A7%A3%E5%86%B3%E6%8F%90%E7%A4%BA%EF%BC%9A%E9%80%9A%E8%BF%87%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%94%B9%E8%BF%9B%E9%9B%B6%E6%A0%B7%E6%9C%AC%E6%80%9D%E7%BB%B4%E9%93%BE%E6%8E%A8%E7%90%86/"/>
    <url>/2024/08/06/%E8%AE%A1%E5%88%92%E5%92%8C%E8%A7%A3%E5%86%B3%E6%8F%90%E7%A4%BA%EF%BC%9A%E9%80%9A%E8%BF%87%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%94%B9%E8%BF%9B%E9%9B%B6%E6%A0%B7%E6%9C%AC%E6%80%9D%E7%BB%B4%E9%93%BE%E6%8E%A8%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h1 id="大语言模型的思维过程"><a href="#大语言模型的思维过程" class="headerlink" title="大语言模型的思维过程"></a>大语言模型的思维过程</h1><p>﻿<a href="https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting">https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting</a><br>﻿最近，大型语言模型 (LLM) 已被证明可以在各种 NLP 任务中提供令人印象深刻的性能。</p><p>为了解决多步骤推理任务，少样本思维链（CoT）提示包括一些手动制作的逐步推理演示，使LLM能够显式生成推理步骤并提高推理任务的准确性。</p><p>为了消除手动工作，Zero-shot-CoT 将目标问题陈述与“让我们一步一步思考”连接起来，作为LLM的输入提示。</p><p>尽管 Zero-shot-CoT 取得了成功，但它仍然存在三个缺陷：计算错误、失步错误和语义误解错误。</p><p>为了解决缺失步骤错误，我们提出了 Planand-Solve (PS) Prompting。它由两个部分组成：首先制定计划，将整个任务划分为更小的子任务，然后根据计划执行子任务。</p><p>为了解决计算错误并提高生成的推理步骤的质量，我们用更详细的指令扩展了 PS 提示，并派生了 PS+ 提示。我们在三个推理问题的十个数据集上评估了我们提出的提示策略。 GPT-3 上的实验结果表明，我们提出的零样本提示在所有数据集上始终大幅优于零样本 CoT，与零样本思维程序提示相当或超过零样本思维程序提示，并且具有可比的性能对数学推理问题进行 8 次 CoT 提示。</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>提示工程的现在与未来</title>
    <link href="/2024/08/05/%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E7%9A%84%E7%8E%B0%E5%9C%A8%E4%B8%8E%E6%9C%AA%E6%9D%A5/"/>
    <url>/2024/08/05/%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E7%9A%84%E7%8E%B0%E5%9C%A8%E4%B8%8E%E6%9C%AA%E6%9D%A5/</url>
    
    <content type="html"><![CDATA[<p><a href="https://mp.weixin.qq.com/s/jd0BU_vUSFvswdsQP2nFDQ">https://mp.weixin.qq.com/s/jd0BU_vUSFvswdsQP2nFDQ</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>通过指令原则提升大模型输出质量</title>
    <link href="/2024/08/01/%E9%80%9A%E8%BF%87%E6%8C%87%E4%BB%A4%E5%8E%9F%E5%88%99%E6%8F%90%E5%8D%87%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BA%E8%B4%A8%E9%87%8F/"/>
    <url>/2024/08/01/%E9%80%9A%E8%BF%87%E6%8C%87%E4%BB%A4%E5%8E%9F%E5%88%99%E6%8F%90%E5%8D%87%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BA%E8%B4%A8%E9%87%8F/</url>
    
    <content type="html"><![CDATA[<p>大语言模型如ChatGPT在多个领域和任务中展现出卓越的能力，但在普通用户设计最优指令或提示时，它们的应用和使用有时可能并不清晰。而他们的工作是为开发人员或普通用户揭示与LLMs询问和交互时时“神秘的黑盒”，并通过简单地策划更好的提示来进一步提高预训练LLMs的响应质量。</p><p>研究团队提出了<strong>26条用于 LLM 提示的原则：</strong></p><ol><li><p>与LLM交流时，无需使用礼貌性语言，如“请”、“如果你不介意”、“谢谢”等，<strong>直接陈述要点</strong>。</p></li><li><p>在提示中<strong>整合预期的听众</strong>，例如“听众是该领域的专家”。</p></li><li><p><strong>将复杂任务分解为一系列更简单的提示，并通过互动式对话进行</strong>。</p></li><li><p><strong>使用肯定的指令</strong>，如“做”，避免使用否定语言，如“不要”。</p></li><li><p>当你需要清晰理解某个主题、想法或任何信息时，使用以下提示：简单地解释[具体主题]。像我是11岁的孩子一样向我解释。像我是[领域]新手一样向我解释。</p></li><li><p>添加“I’m going to tip $xxx for a better solution!”（我会给xxx小费以获得更好的解决方案！）。</p></li><li><p>实施<strong>示例驱动的提示</strong>（使用少数示例提示）。</p></li><li><p>在格式化提示时，首先使用‘###Instruction###’，然后是‘###Example###’或‘###Question###’（如果相关），然后呈现内容。<strong>使用一个或多个换行符来分隔指令、示例、问题、上下文和输入数据</strong>。</p></li><li><p>加入以下短语：<strong>“Your task is”（你的任务是）和“You MUST”（你必须）</strong>。</p></li><li><p>加入以下短语：“You will be penalized”（你将受到惩罚）。</p></li><li><p>在提示中使用短语<strong>“Answer a question in a natural human-like manner”（以自然的人类方式回答问题）</strong>。</p></li><li><p>使用引导性词汇，如写<strong>“think step by step”（逐步思考）</strong>。</p></li><li><p>在你的提示中添加以下短语：“Ensure that your answer is unbiased and does not rely on stereotypes”（<strong>确保你的回答是无偏见的，不依赖于刻板印象</strong>）。</p></li><li><p>允许模型通过向你提问直到获得足够的信息来提供所需输出，例如“From now on I would like you to ask me questions to…”（**从现在开始，我希望你向我提问，直到…**）。</p></li><li><p>要了解特定主题或想法或任何信息，并且你想测试你的理解，你可以使用以下短语：“Teach me the [Any theorem&#x2F;topic&#x2F;rule name] and include a test at the end but don’t give me the answers and then tell me if I got the answer right when I respond”（教我[任何定理&#x2F;主题&#x2F;规则名称]并在最后包括一个测试，但不要给我答案，然后在我回答时告诉我是否正确）。</p></li><li><p>为大语言模型<strong>分配一个角色</strong>。</p></li><li><p><strong>使用分隔符</strong>。</p></li><li><p>在提示中<strong>多次重复特定单词或短语</strong>。</p></li><li><p>结合<strong>思维链（CoT）与少数示例提示</strong>。</p></li><li><p>使用输出引导器，它涉及<strong>以期望输出的开始结束你的提示</strong>。通过以预期响应的开始结束你的提示来使用输出引导器。</p></li><li><p>要编写详细的[论文&#x2F;文本&#x2F;段落&#x2F;文章]或任何需要详细的文本类型：“为我详细写一篇关于[主题]的详细[论文&#x2F;文本&#x2F;段落]，添加所有必要的信息”。</p></li><li><p>要更正&#x2F;更改特定文本而不改变其风格：“尝试修改用户发送的每个段落。你只应该改善用户的语法和词汇，确保它听起来自然。你不应该改变写作风格，例如将正式段落变得非正式”。</p></li><li><p>当你有一个可能涉及不同文件的复杂编码提示时：“**从现在开始，每当你生成跨越多个文件的代码时，生成一个[编程语言]脚本，可以运行以自动创建指定的文件或对现有文件进行更改以插入生成的代码。[你的问题]**”。</p></li><li><p>当你想使用特定单词、短语或句子开始或继续文本时，使用以下提示：我为你提供了开始[歌词&#x2F;故事&#x2F;段落&#x2F;论文…]：[插入歌词&#x2F;单词&#x2F;句子]。根据提供的词汇完成它。保持流畅一致。</p></li><li><p><strong>明确陈述模型必须遵循的要求，以便根据关键词、规则、提示或指令产生内容</strong>。</p></li><li><p>要写任何文本，如文章或段落，且内容与提供的样本相似，请包含以下指示：”<strong>请根据提供的段落[&#x2F;标题&#x2F;文本&#x2F;文章&#x2F;答案]使用相同的语言基础。</strong>“</p></li></ol><p>根据这些原则特有的性质，研究团队将它们分为五个类别：</p><p>(1) <strong>提示结构和清晰度</strong>，例如，在提示中整合预期的听众，如听众是该领域的专家；</p><p>(2) <strong>具体性和信息</strong>，例如，在你的提示中添加以下短语“确保你的回答是无偏见的，不依赖于刻板印象。”；</p><p>(3) <strong>用户互动和参与</strong>，例如，允许模型通过向你提问来获取精确的细节和要求，直到它有足够的信息来提供所需的输出“从现在开始，我希望你向我提出问题…”；</p><p>(4) <strong>内容和语言风格</strong>，例如，与LLM交流无需礼貌，因此无需添加诸如“请”、“如果你不介意”、“谢谢”、“我想要”等短语，直接切入主题；</p><p>(5) <strong>复杂任务和编码提示</strong>，例如，将复杂任务分解为一个互动对话中的一系列更简单的提示。</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>uplift与因果推断</title>
    <link href="/2024/07/30/uplift%E4%B8%8E%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD/"/>
    <url>/2024/07/30/uplift%E4%B8%8E%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD/</url>
    
    <content type="html"><![CDATA[<p>在通常的预测任务中，我们拟合的实际是Y与X的相关关系，X甚至可以是Y的结果，如GDP和发电量之间可能有一系列复杂的关系，但只要二者相关就可以互相预测。在另一些场景中则有所区别，如预测任务要指导干预(Treatment)决策时，我们所能掌控的只有Treatment变量，此时我们希望知道的是执行干预与否的效果差异(通常看增量，uplift)，目的是决策是否执行或执行何种干预。如在“发券&amp;下单”的问题中，用户的历史订单数对下单率预估有较大帮助，但对是否发券的指导意义可能会大打折扣。</p><h2 id="相关、因果、辛普森悖论"><a href="#相关、因果、辛普森悖论" class="headerlink" title="相关、因果、辛普森悖论"></a>相关、因果、辛普森悖论</h2><p>因果关系要求“原因”先于并导致“结果”，而相关关系对顺序不做要求。参考材料中提到了很多示例，如“溺水死亡人数与冰激凌销量正相关”，显然二者不是因果关系，而是由“气温(或季节)”联系起来的相关关系。<br><strong>辛普森悖论(Simpson Paradox)</strong><br>趋势出现在几组数据中，但当这些组被合并后趋势消失或反转。<br>案例：<br>总共的志愿者有700个人（相当于小白鼠），分为两个组，第一组给350个人服用新生产的药物，第二组给另外350个人不用药物（或者说服用糖之类的东西，俗称安慰剂）。服药的第一组350个人中，男性患者87位，女性患者350-87 &#x3D;263位。未服药的第二组350个人中，男性患者270位，女性患者350-270&#x3D;80位，实验结果如表所示<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20240730165623.png" alt="image.png"><br>总数据中，服药的患者痊愈率是78%，未服药的患者痊愈率是83%<br>但是在服药的患者中，男性患者的痊愈率是93%，而未服药的男性患者痊愈率是87%，证明药物有效；女性服药的患者痊愈率是73%，而未服药的患者痊愈率是69%，药物同样有效。<br>整体的效果而言，竟然是不服药的效果好。<br><strong>从数据的角度来说</strong>，此次服用药物患者的350人和不服用药物的350人中，男女比例是不一样的。</p>]]></content>
    
    
    <categories>
      
      <category>因果推断</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>序列模型，自注意力，transformer</title>
    <link href="/2024/07/25/%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%8Ctransformer/"/>
    <url>/2024/07/25/%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%8Ctransformer/</url>
    
    <content type="html"><![CDATA[<p><a href="https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI">https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI</a></p><h3 id="从递归的关系开始"><a href="#从递归的关系开始" class="headerlink" title="从递归的关系开始"></a>从递归的关系开始</h3><p>如何定义一个关系，将网络在特定时间步长的计算与来自先前时间步长的历史记忆联系起来?<br>我们简单地将网络所理解的计算和信息通过我们称之为递归关系的方式链接到其他副本。这意味着在特定时间的网络计算的某些方面被传递给稍后的时间步长。<br>这意味着网络的输出、预测和计算不仅是输入数据X 的函数，而且还有这个其他变量H。它捕获了状态的概念，捕获了网络计算和传递在时间上的记忆概念。</p><h3 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h3><h3 id="为神经网络编码自然语言"><a href="#为神经网络编码自然语言" class="headerlink" title="为神经网络编码自然语言"></a>为神经网络编码自然语言</h3><p>独热编码-&gt;embedding</p><h3 id="处理可变长度序列数据"><a href="#处理可变长度序列数据" class="headerlink" title="处理可变长度序列数据"></a>处理可变长度序列数据</h3><p><a href="https://mp.weixin.qq.com/s?__biz=MzIxODUzNTg2MA==&mid=2247484925&idx=1&sn=f2ff4e9bc8086e82db7f624195606b8b&chksm=97e84726a09fce30b8a09dcb14b4380895ce7248b80dd5e96407d90e3ca94d5fc37cc95e3ec6&scene=21#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzIxODUzNTg2MA==&amp;mid=2247484925&amp;idx=1&amp;sn=f2ff4e9bc8086e82db7f624195606b8b&amp;chksm=97e84726a09fce30b8a09dcb14b4380895ce7248b80dd5e96407d90e3ca94d5fc37cc95e3ec6&amp;scene=21#wechat_redirect</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>提示词工程PLUS</title>
    <link href="/2024/07/23/%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8BPLUS/"/>
    <url>/2024/07/23/%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8BPLUS/</url>
    
    <content type="html"><![CDATA[<h3 id="prompt"><a href="#prompt" class="headerlink" title="prompt"></a>prompt</h3><p>当前大模型是没有“心算”能力的，无法探知自然语言背后的“隐含信息”，对于中文隐含信息量又是及其庞大的。但我们可以通过提示词让模型将心算能力摆在前台，通过上下文将隐含信息摆在前台。<br><strong>举个例子：</strong></p><ul><li><p>36 * 12等于几，人类通过心算时大部分会拆成36<em>10&#x3D;360放在心里某个位置，然后在算36</em>2&#x3D;72放在心里某个位置，最终相加得出结果，而模型没有地方可以放这个临时值</p><ul><li>这种情况下可以在提示词上说：请分步给出答案，通过step by step的方式让模型有放临时值的“地方”</li></ul></li><li><p>“老王儿子明天摆宴席，让我上台讲两句，我该说点什么词？”，在这个提问背后如果是人与人之间的沟通它背后隐含着老王儿子和提问者是什么关系、摆的什么宴席（结婚、升学…..）可能马上就知道怎么回事，但是模型是无法探知的</p><ul><li>这种情况下可以在提示词上说：老王儿子是我外甥，明天要摆升学宴，通过给出隐含信息让模型输出结果更精准</li></ul></li></ul><h3 id="ReAct"><a href="#ReAct" class="headerlink" title="ReAct"></a>ReAct</h3><p>ReAct 是 Reasoning 和 Acting 的缩写。这个框架的基本思想是给一个 Prompt，这个 Prompt 将 Question 拆成几个步骤。分别是：</p><ul><li><strong>Tought</strong>： 面对这个 Question 我下一步应该做什么。</li><li><strong>Action</strong>：执行某个动作。在 ReAct 里有三种动作，第一个是 Search[entity] 如果存在对应实体的维基页面，则返回前5句话，否则使用<a href="https://www.zhihu.com/search?q=%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2984412161%7D">维基百科</a>搜索引擎搜索前5个类似的实体；第二个是 Look[string] 它将返回包含该字符串的页面中的下一句话，模拟浏览器上的 Ctrl+F 功能。第三个是 Finish[answer] 它将使用答案完成当前任务。</li><li><strong>Observation</strong>：观察到的外部工具给到的结果，将作为新的提示输入给 ChatGPT</li></ul><h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><p><a href="https://chunfei-he.github.io/2024/08/10/%E5%9F%BA%E4%BA%8Eembeddings-GPT%E7%9A%84Q&A%E6%8E%A2%E7%B4%A2/">基于embeddings+GPT的Q&amp;A探索 - blog (chunfei-he.github.io)</a></p><h3 id="Langchain"><a href="#Langchain" class="headerlink" title="Langchain"></a>Langchain</h3>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>漫谈大模型中的训练</title>
    <link href="/2024/07/19/%E6%BC%AB%E8%B0%88%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%AE%AD%E7%BB%83/"/>
    <url>/2024/07/19/%E6%BC%AB%E8%B0%88%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%AE%AD%E7%BB%83/</url>
    
    <content type="html"><![CDATA[<h3 id="pretraining，SFT-RLHF"><a href="#pretraining，SFT-RLHF" class="headerlink" title="pretraining，SFT, RLHF"></a>pretraining，SFT, RLHF</h3><p>1.Pretraining:第一阶段预训练所需的语料是各种类型的世界知识，包括网页、书籍、新闻、论文期刊、对话文本、代码等形式，通过大量学习世界知识，构建模型的基础能力，使得模型能够“漂亮地说话”。该阶段的语料特征可以概括为“广“。给模型海量的文本进行训练，99%的计算量花费在这个阶段，输出的模型叫做base model，能做的事情就是像成语接龙一样不断的完成一段话；2.SFT(Supervised Fine-tuning):第二阶段SFT,通过标注人员设计问答，编写正确答案，将例题投喂给模型，并希望模型在没有见过的任务中”举一反三”，提升泛化能力。通俗的讲，就是人工介入，给出高质量的文本问答例子。经过问答式训练的Mode叫做SFT model,就可以正常回答人的问题了；<br>3.RLHF(Reinforcement Learning from Human Feedback):第三阶段RLHF,训练目标是让模型的价值观与人类对齐，需要人类对模型的回答进行打分、排序，让模型知道”怎么说更好”。第二和第三阶段的数据质量要求较高，需要来自人类的高质量反馈，语料特征可以概括为“齐“。第三阶段人工先介入，通过对同一个Prompt生成答案的排序来训练一个Reward Model。.再用Reward Model:去反馈给SFT Model,,通过评价生成结果的好坏，让模型更倾向于生成人们喜好的结果。最终生成的Modell叫做RLHF model,</p><p><a href="https://bce.bdstatic.com/p3m/common-service/uploads/SFT-8_fec7948.mp4">https://bce.bdstatic.com/p3m/common-service/uploads/SFT-8_fec7948.mp4</a><br><a href="https://www.youtube.com/watch?v=bZQun8Y4L2A">https://www.youtube.com/watch?v=bZQun8Y4L2A</a><br><a href="https://www.youtube.com/watch?v=zjkBMFhNj_g&t=937s">https://www.youtube.com/watch?v=zjkBMFhNj_g&amp;t=937s</a></p><p>数据长度95%以上在4k以内：4k版本<br>超过4k的数据较多（20%以上）或需长文本处理：8k版本</p><p>全量更新：更新模型所有参数<br>lora：一小部分参数调整<br>prompt tuning：预训练参数不变，优化提示词</p><p>样本数量少于1000且需注重大模型本身的通用能力：lora<br>特定任务数据样本较多且主要关注这些特定任务的效果：全量</p><p>llama-2:<br>epoch：100条数据epoch为15，1000条为10，10000条为2（特殊任务，如自然语言生成sql除外）<br>batchsize越大越好</p><p>如果模型只会回答训练见过的内容，可能是过拟合<br>成功的训练一般training loss和perlexity有明显收敛过程，出现在后半部分，下降且平稳</p><p>rouge<br>bleu<br>看答案的文本相似度，在简单任务中，回答准会比较高，100左右；复杂任务如写故事，50-60就很高了。</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型实战-函数调用</title>
    <link href="/2024/07/13/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98-%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8/"/>
    <url>/2024/07/13/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98-%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb">https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb</a><br>Chat聊天：纯粹的Chat,像是一个主要由“大脑和嘴”构成的智能体，专注于信息处理和语言交流。比如ChatGPT这样的系统，它能够理解用户的查询，给出有用和连贯的回答，但它本身不直接执行任务。<br>Agent代理：像一个具有“手、脚”的智能体，它能够进行思考、决策，并且能执行具体的任务</p><p>总结一下，这里面的流程步骤：<br>1.用户把问题和函数功能交给openai<br>2.openai通过json格式返回能处理的问题（比如，一个模糊的地点，转换为准确的城市和<br>省)<br>3.用户通过返回的具体答案，调用自己的数据库，或是第三方api,获取json格式的数据<br>4.把上下文和返回的json数据汇总给openai<br>5.openai通过自然语言返回用户提出的问题</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型实战-文本分类和聚类</title>
    <link href="/2024/07/05/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E5%92%8C%E8%81%9A%E7%B1%BB/"/>
    <url>/2024/07/05/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E5%92%8C%E8%81%9A%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/629024155">https://zhuanlan.zhihu.com/p/629024155</a></p><h3 id="openai的completion与embedding接口"><a href="#openai的completion与embedding接口" class="headerlink" title="openai的completion与embedding接口"></a>openai的completion与embedding接口</h3><p>1.Completion可以让模型根据用户的输入进行目动续写，Completion接口，一万面可以直接当作天机器人使用，另一方面，只要善用prompt提示词，就能完成文案撰写、文本摘要、机器翻译等工作2.Embedding可以将用户输入的文本转化成向量。Embedding向量适合作为一个中间结果，用于传统的机器学习场景，比如分类、聚类。</p><p>OpenAI 支持批量调用接口，可以在一个请求里一次批量处理很多个请求。通过将 1000 条记录打包在一起处理，速度将会快很多。<br>对于大数据集，我们不应该存储成 CSV 格式。特别是获取到的 Embedding 数据，它是由很多浮点数组成的。如果存储成 CSV 格式，那么它会把本来只需要 4 个字节的浮点数都用字符串的形式存储下来，这会浪费好几倍的空间，并且写入的速度也会变得很慢。因此，我在这里采用了 parquet 这个序列化的格式。使用 parquet 格式可以节省空间并提高写入速度，整个存储的过程只需要 1 分钟左右。另外，为了确保数据的安全，我们还可以对 parquet 文件进行加密和压缩，这样可以进一步减小存储空间，并且保护数据的机密性。</p><h3 id="指标含义"><a href="#指标含义" class="headerlink" title="指标含义"></a>指标含义</h3><p>准确率表示模型正确判断为该类别的标题所占的比例，即在所有判断为该类别的标题中有多少是真正属于该类别。举个例子，模型判断有100个标题属于农业新闻，但实际上只有83个标题是农业新闻，那么准确率就是0.83。准确率越高越好，但是并不意味着准确率达到100%就代表模型完全正确，因为模型可能会漏判，所以我们还需要考虑召回率。</p><p>召回率表示模型正确判断为该类别的标题占实际该类别下所有标题的比例，即没有漏掉的比例。例如，模型判断有100个标题属于农业新闻，这100个标题实际上都是农业新闻。准确率已经达到100%，但是实际上我们共有200条农业新闻。因此，在农业新闻类别中，我们的召回率只有100&#x2F;200 &#x3D; 50%。</p><p>因此，评估模型效果时需要考虑准确率和召回率，综合考虑这两个指标得出的结果就是F1分数。F1分数是准确率和召回率的调和平均数，即 F1 Score &#x3D; 2 * (Precision * Recall) &#x2F; (Precision + Recall)。当准确率和召回率都为100%时，F1分数也为1。如果准确率为100%，召回率为80%，那么计算得到的F1分数为0.88。F1分数越高越好。</p><p>支持样本量表示数据中实际属于该类别的样本数量。一般来说，样本数量越多，该类别的训练结果就越准确。</p><p>accuracy：总体上判断正确的分类数除以测试样本数，即模型的整体准确率。<br>macro average是宏平均，它将每个类别计算得到的指标加在一起取平均。宏平均对于数据分类不平衡的情况非常有用。比如，假设我们进行情感分析，其中90%的样本属于正面情感，而10%的样本属于负面情感。在这种情况下，如果我们的模型在正面情感方面的预测效果很好，准确率达到了90%，但在负面情感方面的准确率只有50%。如果只看整体数据，准确率似乎很高，因为正面情感的样本很多。但是对于我们的目标来说，即找到具有负面情感的客户并与他们沟通、进行赔偿，整体准确率就没有什么用了。而宏平均会将整体准确率计算为(90% + 50%)&#x2F;2 &#x3D; 70%。这并不是一个很好的预测结果，我们需要进一步优化模型。宏平均在处理数据样本不平衡的情况下非常有用</p><h3 id="文本聚类"><a href="#文本聚类" class="headerlink" title="文本聚类"></a>文本聚类</h3><p>K-Means:是一种无监督的聚类算法，其中的K代表类簇个数，Means代表类簇内数据对象的均值。算法基本原理是：先从样本集中随机选取K个样本作为簇中心，并计算所有样本与这K个“簇中心”的距离；对于每一个样本，将其划分到与其距离最近的“簇中心”所在的簇中；然后再计算每个簇的新的“簇中心”。迭代进行上述过程，直到达到某个终止条件（如迭代次数、簇中心的移动距离小于某个阈值等）为止。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">1</span> <span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-number">2</span> <span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans<br><span class="hljs-number">3</span><br><span class="hljs-number">4</span> embedding_df pd.read_parquet(<span class="hljs-string">&quot;data/20newsgroups.parquet&quot;</span>)<br><span class="hljs-number">5</span><br><span class="hljs-number">6</span> matrix np.vstack(embedding_df.embedding.values)<br>7num_of_clusters =<span class="hljs-number">20</span><br><span class="hljs-number">8</span><br><span class="hljs-number">9</span> kmeans KMeans(n_clusters=num_of_clusters,init=<span class="hljs-string">&quot;k-means++&quot;</span>,n_init=<span class="hljs-number">10</span>, random_state=<span class="hljs-number">42</span>)<br><span class="hljs-number">10</span> kmeans.fit(matrix)<br><span class="hljs-number">11</span> labels kmeans.labels_<br><span class="hljs-number">12</span> embedding_df[<span class="hljs-string">&quot;cluster&quot;</span>]=labels<br><span class="hljs-number">13</span><br><span class="hljs-number">14</span><span class="hljs-comment">#统计每个clusteri的数量</span><br><span class="hljs-number">15</span> new_df embedding_df.groupby(<span class="hljs-string">&#x27;cluster&#x27;</span>)[<span class="hljs-string">&#x27;cluster&#x27;</span>].count().reset_ind<br><span class="hljs-number">16</span><span class="hljs-comment">#输出结果</span><br><span class="hljs-number">17</span> <span class="hljs-keyword">from</span> IPython.display <span class="hljs-keyword">import</span> display<br><span class="hljs-number">18</span> display(new_df)<br></code></pre></td></tr></table></figure><p>上述代码通过numPy的stack函数，把所有的Embedding)放到一个矩阵里面，并设置了要聚<br>合出来的类的数量，最后调用了一下K-Means:算法的ft方法。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>token介绍</title>
    <link href="/2024/06/30/token%E4%BB%8B%E7%BB%8D/"/>
    <url>/2024/06/30/token%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p><a href="https://platform.openai.com/tokenizer">https://platform.openai.com/tokenizer</a><br>OpeA的大型语言模型（即GPT)使用标记处理文本，标记是一组文本中常见的字符序列。模型学习理解这些标记之间的统计关系，并擅长生成标记序列中的下一个标记。您可以使用相关工具来了解语言模型如何对一段文本进行标记，以及该段文本中标记的总数</p><h3 id="token是什么"><a href="#token是什么" class="headerlink" title="token是什么"></a>token是什么</h3><p>在自然语言处理(NLP)中，token是指一组相关的字符序列，例如一个单词或一个标点符号。将文本分解为token是NLP的一项基本任务，因为它是许多其他任务的先决条件，例如词性标注、命名实体识别和机器翻译。在文本处理中，token可以是一个词语、数字、标点符号、单个字母或任何可以成为文本分析的单个元素。</p><p>在分解文本时，通常会根据空格、标点符号和其他特定的分割符号来确定token的边界。<br>例如，在以下句子中，标点符号和空格用于分解成为不同的token:<br>“我喜欢吃冰淇淋。”<br>这个句子中，每个汉字和标点符号都可以切分开成一个tokn。但是，一个字一个字去理解整句话的意思，可能反而会理解错误。<br>例如“冰淇淋”，就是一个完整的词，分开成“冰”“淇”“淋”三个字反而无法理解了。<br>类似的，NLP中，token还可以是比词更高级别的语言单位，例如短语或句子。例如，对于短语token,“红色的苹果”可以被视为一个token,而不是单独的“红色”和“苹果”token.。因为存在不同的切分方式，所以“红色的苹果”，就需要切分成“红”“红色”的”苹果””“果”“红色的苹果”等多个token:去理解。<br>在处理文本时，理解token的概念是非常重要的，因为它是许多文本分析任务的基础。NLP算法会使用token来构建文本的表示形式，理解自然语言，以便进行其他分析任务。<br>因此，对于NLP系统来说，选择正确的分词方法(tokenization)非常重要，它将直接影响到其他任务的准确性和效率。<br>我们结合Chatgpt进一步了解下token和tokenization。</p><h3 id="ChatGPT中的token"><a href="#ChatGPT中的token" class="headerlink" title="ChatGPT中的token"></a>ChatGPT中的token</h3><p>一句话输入大模型之后后先拆分成token，ChatGPT的词表一共有100256个不同符号（BPE格式）。<br>常见单词对应的token索引id小。如果一个单词单独出现时被拆分成了两个token，但是前面加上空格后可以分配到一个token，说明该单词在段落中间出现的概率高。<br>token 其实还包括了一些空白字符，因此在边界容易出问题，这是在应用时需要注意的。<br>以前面「Once upon a time」的例子说明，”Once “（后面有空格）的 token id 是 <code>[7454, 220]</code>，其中空格的 id 是 220，但如果是 <code>&quot;Once upon&quot;</code>，token id 就是 <code>[7454, 2402]</code>，其中 <code>&quot; upon&quot;</code>（前面有空格） 的 id 是 2402，而 “upon” 这个不加前面空格的单词 token 是 27287，前面提到过 id 值大意味着概率低，在大模型眼里，提示词结尾加不加空格是完全不同，加了就是用 <code>[7454, 220]</code> 预测 27287，不加就是用 <code>[7454]</code> 预测 2402，第一种是概率更低更难的，因此在写提示词的时候<strong>结尾不要加空格</strong></p><p><a href="https://www.youtube.com/watch?v=zduSFxRajkE">https://www.youtube.com/watch?v=zduSFxRajkE</a></p><p>看完了 token，接下来介绍选词，前面提到大模型最终输出结果是每个 token 的概率，要选择哪个 token 作为下一个词呢？最简单的想法是选择概率最大的，但这样并不好，一方面是单次概率最大不代表全局最优，另一方面导致大模型同一个问题每次输出结果都一样，对于创意类的场景不合适。因此目前的做法是根据输出概率来做采样，概率高的 token 更容易输出。</p><p>在实际应用的时候，大模型通常会有个可调整的参数叫 <code>Temperature</code>，它可以控制大模型输出结果更稳定还是更多样，它是怎么实现的呢？这里用代码来解释一下，比如我们假设大模型输出了这个向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">output = torch.FloatTensor([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.5</span>])<br></code></pre></td></tr></table></figure><p>通过 softmax 函数我们可以将输出结果转成总和为 1 的小数，每个小数就是输出概率，这就是大模型最后算出的结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.nn.functional.softmax(output, -<span class="hljs-number">1</span>) <br><span class="hljs-comment"># 输出为 [0.1621, 0.1792, 0.1980, 0.2188, 0.2419]</span><br></code></pre></td></tr></table></figure><p>比如第五个值被选中的概率是 24.19%，这里每个数组的索引就是前面提到的 token id，第五个值代表 token id 为 5。</p><p>如果我们将输出结果除以 0.1，结果就变成如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">temperature = <span class="hljs-number">0.1</span><br>torch.nn.functional.softmax(output / temperature, -<span class="hljs-number">1</span>)<br><span class="hljs-comment"># 输出为 [0.0117, 0.0317, 0.0861, 0.2341, 0.6364]</span><br></code></pre></td></tr></table></figure><p>这时第五个值的概率变成 63.64% 了，被选中的概率大幅增加，输出结果更为稳定</p><p>而如果是除以 2，就变成：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">temperature = <span class="hljs-number">2</span><br>torch.nn.functional.softmax(output / temperature, -<span class="hljs-number">1</span>)<br><span class="hljs-comment"># 输出为 [0.1805, 0.1898, 0.1995, 0.2097, 0.2205]</span><br></code></pre></td></tr></table></figure><p>这时各个列的输出概率就被压平了，第五个值的的概率变成 22%，而之前第一个值的概率从 1% 变成了 18.05%，比之前更有可能被选中了，这就使得大模型输出结果更多样，也意味着跟容易瞎说。<br>因此将 <code>temperature</code> 值设小一点模型就能很稳定输出，另外 <code>temperature</code> 是被除数，所以不可以为 0，有些平台支持 0 是做了特殊处理，比如可以转成 <code>top_k</code> 为 1。</p><p>不过改成 0 也不能完全保证结果唯一，根据 OpenAI 员工 <a href="https://link.zhihu.com/?target=https://community.openai.com/t/a-question-on-determinism/8185">boris</a> 的说法，GPU 浮点数计算的时候有不确定性，而且多个 GPU 推理时 <code>a*b*c</code> 可能被计算为 <code>(a*b)*c</code> 或 <code>a*(b*c)</code>，这两个结果可能会微小不同，导致最终结果不唯一。</p><p>前面花了很多篇幅介绍 token 及选词策略，因为这是在大模型应用时控制输出的重要机制，在不改变大模型参数的情况下，我们可以：</p><ol><li>提升推理速度，比如 <a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2302.01318">Speculative Sampling</a>，它的原理是用个小模型来生成一段，再让大模型来挑选是否可用，这个方式能提升 2-3 倍推理速度。</li><li>优化推理效果，比如使用 <a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Beam_search">Beam search</a> 来搜索更好的结果，它通过多次搜索来找到更优结果，但会牺牲性能，因此实际应用中比较少见。</li><li>控制输出格式，在应用中我们通常需要大模型输出 JSON 格式，而大模型有时候会幻觉导致输出错误，这时可以在解码时进行干预，比如发现新 token 会导致 JSON 语法错误就自动修正，比起全部输出后再修正效果更好。</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GPT名称含义</title>
    <link href="/2024/06/25/GPT%E5%90%8D%E7%A7%B0%E5%90%AB%E4%B9%89/"/>
    <url>/2024/06/25/GPT%E5%90%8D%E7%A7%B0%E5%90%AB%E4%B9%89/</url>
    
    <content type="html"><![CDATA[<h3 id="generative"><a href="#generative" class="headerlink" title="generative"></a>generative</h3><p>Generative这个单词，揭示了Chat Gpt的本质就是一个词语接龙器。<br>比如，问：今天天气很糟。<br>GPT的工作是步骤是：<br>1)检查句子是不是完整。假如我问的是：“今年天气很”，那么GPT的回答很可能是：很抱<br>歉，您的信息不完整。<br>2)把“今天天气很糟”作为输入，从众多中文字里面选出一个频率最高的单词，这里是“很”；<br>3)然后把“今天天气很糟，很作为输入，得到今天天气很糟，很遗憾”；<br>4)重复2、3两步，直到碰到一个“END”结束。<br>GPT一定是一个字一个字崩出来的，我们在网页版的时候会看到答案输出是一个字一个字来<br>的。<br>GPT按照事前设置的模型，套出后文，而这个模型就是pre-trained出来的。</p><h3 id="pre-trained"><a href="#pre-trained" class="headerlink" title="pre-trained"></a>pre-trained</h3><p>预训练又称为学习，有三种模式：无监督学习、监督学习、增强学习。<br>GPT起先是无监督学习，就是喂给机器各种文本，在1.0，喂的数据是1GB;到了2.0这个数<br>据是40GB;而到了GPT3,是570GB。<br>无监督学习的目的，就是根据“公式”算出下一个出现的词的概率，然后选择排名靠前的词。<br>既然公式，那就必然有参数，比如，f(x)&#x3D;ax+b,这里的a和b就是参数。我们知道，语言是<br>一个很复杂的表达，根据前文要能算出后文，这个公式就会很复杂，牵涉到的参数会很多。<br>具体是：GPT1.0117M参数，到了2.0是1542M参数，而到了GPT3是175B的参数。理论上<br>讲，参数越多，输出的结果就越准确。<br>但是，仅仅是无监督学习是完全不够的。<br>比如：<br>如果问7+9&#x3D;，机器很可能会输出（)，因为，它接受的训练里面，大量的句子就是7+9&#x3D;<br>()<br>很正确，但是这不是我们想要的答案。<br>这时候就需要引入“导师”，告诉它这个答案不对，应该回答16，这需要人工干预，纠正机器<br>的错误。<br>当然，这个所谓的“人工干预”，通常不是“人肉干预”，会有另外一段代码去干预，比如的计<br>算器。这个过程就是监督学习了。<br>对于有确定答案的问句，监督学习当然很好。但是有些问题是没有标准答案的，比如：<br>给我写首李白风格的诗。<br>这时候需要“增强学习”，增强学习告诉机器的不是正确答案，而是给机器点赞。</p><h3 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h3><p>语言，其实就是一个文字序列。<br>所以，问答，就是从一个文字序列到另一个文字序列。人工智能里面有个大名鼎鼎的词：<br>Seq2Seq,说得就是这个算法。<br>两个句子体会下：<br>句子A:小马没能瞠过河流，因为它太累了；<br>句子B:小马没能蹚过河流，因为它太宽了；<br>显然，第一个句子中的它是小马，而第二个句子中的它是河流。<br>一个词的含义和它的前后文都有关系，为了处理这个东西，起先是用的RNN算法循环神经<br>网络(Recurrent Neural Network,.RNW)。各位大佬前赴后继，一通折腾，总算算法搞的<br>比较好了。<br>RNN是个没法并行计算的网络架构，前一个算完了才能算后一个，它唯一问题就是：这玩<br>意很难并行计算。不能并行计算就没多大价值，因为太慢了。<br>直到有一篇论文横空出世，论文题目也是相当霸道，《Attention Is All You Need)》。<br>绝对的超级大招，彻底解决了这个问题。这个模型就是Transformer.。Transformer中存在6<br>层的encode.与6层的decode,每一个decode都直接跟encode连接上，那么6个decode可以<br>直接并行计算，在decode层面速度就可以提升6倍。而GPU可以支持并行处理。<br><a href="https://www.youtube.com/watch?v=zxQyTK8quyY">https://www.youtube.com/watch?v=zxQyTK8quyY</a></p><h3 id="术语整理"><a href="#术语整理" class="headerlink" title="术语整理"></a>术语整理</h3><ol><li>in-context learning 上下文学习<br>在prompt里面写几个例子，模型就可以照着这些例子做生成，即在给定的上下文中使用新的输入来改善模型的输出。这种学习方式并不涉及到梯度更新或微调模型的参数，而是通过提供一些具有特定格式或结构的示例输入，使模型能够在生成输出时利用这些信息。例如，如果你在对话中包含一些英法翻译的例子，然后问模型一个新的翻译问题，模型可能会根据你提供的上下文示例生成正确的翻译</li><li>few-shot learning 少样本学习<br>用极少量的标注样本来训练机器学习模型的技术。在GPT-3的案例中，少样本学习的实现方式是向模型提供少量的输入输出对示例，这些示例作为对话的一部分，描述了模型应该执行的任务。然后模型会生成一个输出，该输出是对与示例类似的新输入的响应。例如，你可以给模型提供几个英法翻译的例子，然后给出一个新的英文单词让模型翻译，模型会尝试产生一个正确的翻译</li><li>prompt engineering 提示工程<br>提示工程是指设计和优化模型的输入提示以改善模型的输出。在大型语言模型中，如何提问或构造输入的方式可能对模型的输出有重大影响。因此，选择正确的提示对于获取有用的输出至关重要。例如，为了让GPT3生成一个诗歌，你可能需要提供一个详细的、引导性的提示，如“写一首关于春天的十四行诗，而不仅仅是“写诗</li><li>prompt 提示词<br>把prompt输入给大模型，大模型给出completion</li><li>instruction tuning 指令微调<br>用instruction 来fine-tune大模型</li><li>code tuning 代码微调<br>用代码来fine-tune大模型</li><li>reinforcement learning with human feedback(RLHF) 基于人类反馈到强化学习<br>让人给模型生成的结果打分，用人打的分来调整模型</li><li>chain of thought 思维链<br>在写prompt时，不仅给出结果，还要一步一步地写结果时怎么推出来的</li><li>scaling laws 缩放法则<br>模型的效果的线性增长要求模型的大小指数增长</li><li>alignment 与人类对齐<br>让机器生成符合人类期望的，符合人类价值观的句子</li><li>emergent ability 突现能力<br>小模型没有，只有模型达到一定规模才会出现的能力</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>embedding</title>
    <link href="/2024/06/24/embedding/"/>
    <url>/2024/06/24/embedding/</url>
    
    <content type="html"><![CDATA[<p>将高维数据映射到低维空间。Embedding的最大价值是降维：在许多实际问题中，原始数据的维度往往非常高。例如，在自然语言处理中，如果使用One-hot编码来表示词汇，其维度等于词汇表的大小，可能达到数十万甚至更高。通过Embedding,我们可以将这些高维数据映射到一个低维空间，大大减少了模型的复杂度。</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>预训练语言模型</title>
    <link href="/2024/06/19/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    <url>/2024/06/19/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h3 id="NLP语言模型发展一览"><a href="#NLP语言模型发展一览" class="headerlink" title="NLP语言模型发展一览"></a>NLP语言模型发展一览</h3><p>深度学习阶段：encoder-decoder&#x2F;word2vec&#x2F;attention<br>预训练模型阶段：transformer&#x2F;bert&#x2F;GPT-2&#x2F;GPT-3，pre- training+fine-tuning<br>大语言模型阶段：GPT3.5&#x2F;GPT4，instruction-tuning&#x2F;prompt-training</p><pre><code class="hljs">深度学习的训练：inference得到预测值与标注数据对比，使用loss function得到差，使用back propagation修改。过程中进行梯度下降（SGD）</code></pre><p>预训练语言模型首先使用大量未标注数据进行自监督学习，学习出的语言模型针对特定的下游任务进行微调。</p><h3 id="prompt-tuning和fine-tuning的区别"><a href="#prompt-tuning和fine-tuning的区别" class="headerlink" title="prompt tuning和fine tuning的区别"></a>prompt tuning和fine tuning的区别</h3><p>prompt tuning会固定预训练模型的参数<br>Prompt Tuning和Fine Tuning虽然都是微调，但是有区别。提示学习(PromptTuning)的灵感源于GPT-3,就是在Prompt中插入一段task specific的可以tune的prompt token。由于这个token对于每个任务都是不同的，所以可以帮助机器识别任务到底是什么。又因为机器自己学习(tune)这个prompt token,所以这个token对于机器会有非常好的效果。</p><p>提出Prompt Tuning的动机是，语言模型（Language Models）越来越大，Fine-tune的成本也越来越高。<br>Fine-tuning的本质是改变预训练模型的weights。由于预训练模型在原域上已经有非常好的性能了，域的迁移会受到原域的阻力，因为使用fine-tune改变的weight是原域上的weight。这样，想要在新域上获得较好的性能，预训练模型越大，fine-tune需要的数据也越多。</p><h4 id="prompt的分类"><a href="#prompt的分类" class="headerlink" title="prompt的分类"></a>prompt的分类</h4><h5 id="硬提示-离散提示（Hard-Prompt-Discrete-Prompt）"><a href="#硬提示-离散提示（Hard-Prompt-Discrete-Prompt）" class="headerlink" title="硬提示&#x2F;离散提示（Hard Prompt&#x2F;Discrete Prompt）"></a>硬提示&#x2F;离散提示（Hard Prompt&#x2F;Discrete Prompt）</h5><p>人为设计<br>根据2021年的两份研究，硬提示有两个性质：</p><ul><li>人类认为不错的硬提示对于LM来说不一定是一个好的硬提示，这个性质被称为硬提示的sub-optimal（次优）性。</li><li>硬提示的选择对于预训练模型的影响非常大。</li></ul><h5 id="软提示-连续提示（Soft-Prompt-Continuous-Prompt）"><a href="#软提示-连续提示（Soft-Prompt-Continuous-Prompt）" class="headerlink" title="软提示&#x2F;连续提示（Soft Prompt&#x2F;Continuous Prompt）"></a>软提示&#x2F;连续提示（Soft Prompt&#x2F;Continuous Prompt）</h5><p>把Prompt的生成本身作为一个任务进行学习，相当于把Prompt的生成从人类一个一个尝试（离散）变换成机器自己进行学习、尝试（连续）。由于需要机器自己学习，软提示不可避免地往模型内引入了新的参数。这里就又出来一个问题：如何<strong>参数有效</strong>地学习软提示？目前的研究热点有：</p><ul><li>P-tuning：将prompt变成token，用BiLSTM进行学习。</li><li>P-tuning：使用混合的prompt初始化策略（如CLInit和SelectInit）。</li><li>Prefix-tuning：对于不同模型，将prompt注入输入的不同位置。</li></ul><h4 id="从Prompt到Prompt-Tuning：让机器学习写Prompt"><a href="#从Prompt到Prompt-Tuning：让机器学习写Prompt" class="headerlink" title="从Prompt到Prompt Tuning：让机器学习写Prompt"></a>从Prompt到Prompt Tuning：让机器学习写Prompt</h4><p><strong><em>所谓Prompt Tuning，就是在Prompt中插入一段task-specific的可以tune的prompt token。</em></strong><br>由于这个token对于每个任务都是不同的，所以可以帮助机器识别任务到底是什么。又因为机器自己学习（tune）这个prompt token，所以这个token对于机器会有非常好的效果。</p><h5 id="参数有效性（Parameter-efficiency）"><a href="#参数有效性（Parameter-efficiency）" class="headerlink" title="参数有效性（Parameter-efficiency）"></a>参数有效性（Parameter-efficiency）</h5><p>所谓“参数有效”，本质就是“节约参数”。怎样算节约参数呢？复用预训练模型越多，节约的参数越多。这是因为Fine-tune预训练模型相当于改变整个预训练模型，而这需要改变非常多的参数。想要进行Transfer Learning的话，改变这么多参数是不能接受的。因此，Prompt Learning的一大好处就叫Parameter Efficient。</p><h3 id="预训练模型三种网络架构"><a href="#预训练模型三种网络架构" class="headerlink" title="预训练模型三种网络架构"></a>预训练模型三种网络架构</h3><p>1.Encoders编码器：编码器主要用于处理和理解输入信息。这些模型可以获得双向的上下文，即可以同时考虑一个词的前面和后面的信息。由于编码器可以考虑到未来的信息，它们非常适合用于需要理解整个句子的任务，如文本分类、命名实体识别等。预训练编码器需要使其能够构建强大的表示，这通常通过预测某个被遮蔽的单词、预测下一个句子或者其它的预训练任务来实现。BERT就是一种典型的预训练编码器，如RoBERTa、ALBERT。百度的ERNIE也跟BERT类似。<br>2.Decoders解码器：解码器主要用于生成输出信息。它们是语言模型的基础，用于预测下一个单词。解码器只能考虑到过去的词，而不能考虑到未来的词，这使得它们非常适合于生成任务，如文本生成、对话系统等。GPT就是一种典型的预训练解码器。如GPT-1、GPT-2、GPT-3三代。所以说GPT名称叫作预训练的解码器也是可以的。<br>3.Encoders-Decoders编码器-解码器：编码器-解码器结构结合了编码器和解码器的优点。编码器首先处理输入信息，然后解码器生成输出信息。这种结构可以考虑到全局的上下文信息，因此非常适合于需要理解输入信息并生成相应的Decoders输出的任务，如机器翻译、文本摘要等。如何预训练编码器解码器模型是一个开放的问题，因为需要考虑到如何有效地使用编码器和解码器的特点。T5和BART都是典型的预训练编码器解码器模型。</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLM调研</title>
    <link href="/2024/03/19/LLM%E8%B0%83%E7%A0%94/"/>
    <url>/2024/03/19/LLM%E8%B0%83%E7%A0%94/</url>
    
    <content type="html"><![CDATA[<h2 id="一-推荐场景"><a href="#一-推荐场景" class="headerlink" title="一 推荐场景"></a>一 推荐场景</h2><p>Xu 等 - 2024 - Prompting Large Language Models for Recommender Sy.pdf</p><h3 id="1-LLM作为推荐模型"><a href="#1-LLM作为推荐模型" class="headerlink" title="1 LLM作为推荐模型"></a>1 LLM作为推荐模型</h3><p>1）判别式推荐discriminative recommendation</p><p>item scoring</p><p>re-ranking tasks</p><p>click-through rate prediction</p><p>[tips]<a href="https://github.com/RUCAIBox/LLMRank">https://github.com/RUCAIBox/LLMRank</a> for questions : (1)很难感知历史交互的顺序，(2)可能会受到提示符中流行度或项目位置的影响。</p><p>2）生成式推荐generative recommendation<br>Li 等 - 2023 - Large Language Models for Generative Recommendatio.pdf</p><p><em>A generative recommender system directly generates recommendations or recommendation-related content without the need to calculate each candidate’s ranking score one by one for sorting and ranking.</em></p><p>分类</p><ul><li><p>non-tuning paradigm</p></li><li><p>tuning paradigm</p></li></ul><p>计算成本高、推理时间慢</p><h3 id="2-LLM改进推荐模型"><a href="#2-LLM改进推荐模型" class="headerlink" title="2 LLM改进推荐模型"></a>2 LLM改进推荐模型</h3><p>1）LLM as feature encoder</p><p>2）LLM for data augmentation</p><p>如通过学习用户过往影视偏好，生成个性化摘要</p><h3 id="3-LLM作为推荐模拟器"><a href="#3-LLM作为推荐模拟器" class="headerlink" title="3 LLM作为推荐模拟器"></a>3 LLM作为推荐模拟器</h3><p>agent映射了真实世界用户的基本行为。</p><h3 id="4-case"><a href="#4-case" class="headerlink" title="4 case"></a>4 case</h3><p>1 </p><p>Xi 等 - 2023 - Towards Open-World Recommendation with Knowledge A.pdf<br>2 </p><p>Yin 等 - 2023 - Heterogeneous Knowledge Fusion A Novel Approach f.pdf<br>3 </p><p>Agent4Rec，一个拥有1000个 LLM 授权生成代理的推荐系统模拟器。这些代理从 MovieLens-1M 数据集初始化，包含不同的社会特征和偏好。每个代理以逐页的方式与个性化电影推荐进行交互，并采取各种行动，如观看、评级、评估、退出和面试。通过 Agent4Rec，我们希望探索 LLM 授权的生成代理在模拟推荐环境中真正独立的人类行为方面的潜力。</p><p>﻿<a href="https://github.com/LehengTHU/Agent4Rec">https://github.com/LehengTHU/Agent4Rec</a>﻿</p><p>5</p><p>使用开源 LLM，我们利用它们的深层作为内容编码器，丰富了内容在嵌入级别的表示。对于闭源 LLM，我们使用提示技术在令牌级别上丰富训练数据。</p><p>﻿<a href="https://github.com/Jyonn/ONCE">https://github.com/Jyonn/ONCE</a><br>﻿## 二 LLM功能归纳</p><p>﻿<a href="https://github.com/TamSiuhin/LLM-UM-Reading">https://github.com/TamSiuhin/LLM-UM-Reading</a>﻿</p><h3 id="LLMs-as-Predictors"><a href="#LLMs-as-Predictors" class="headerlink" title="LLMs as Predictors"></a>LLMs as Predictors</h3><h3 id="LLMs-as-Enhancer"><a href="#LLMs-as-Enhancer" class="headerlink" title="LLMs as Enhancer"></a>LLMs as Enhancer</h3><h3 id="LLMs-as-Controllers-Evaluators"><a href="#LLMs-as-Controllers-Evaluators" class="headerlink" title="LLMs as Controllers &amp;&amp; Evaluators"></a>LLMs as Controllers &amp;&amp; Evaluators</h3><h3 id="Applications-of-LLM-UM"><a href="#Applications-of-LLM-UM" class="headerlink" title="Applications of LLM-UM"></a>Applications of LLM-UM</h3><p><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/Pasted%20image%2020240815161156.png" alt="Pasted image 20240815161156.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>推荐</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>agent for recommendation</title>
    <link href="/2024/03/15/agent-for-recommendation/"/>
    <url>/2024/03/15/agent-for-recommendation/</url>
    
    <content type="html"><![CDATA[<h3 id="When-Large-Language-Model-based-Agent-Meets-User-Behavior-Analysis-A-Novel-User-Simulation-Paradigm"><a href="#When-Large-Language-Model-based-Agent-Meets-User-Behavior-Analysis-A-Novel-User-Simulation-Paradigm" class="headerlink" title="When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm"></a>When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm</h3><p>a profiling module, a memory module and an action module</p><ul><li><p>用户配置文件包括ID、姓名、性别、年龄、特征、职业、兴趣和行为特征。</p><ul><li>（Feature：为了更准确地描述推荐域中的用户，我们使用ChatGPT来总结在线用户的五种显著行为特征，并将其纳入用户简档中。包括：</li></ul></li></ul><p>“(1) Watcher: the users with this feature will actively provide feedback and ratings to the interacted items. 将主动对交互项目提供反馈和评级</p><p>(2) Explorer: the users with this feature will actively search for items they have heard before, and produce detailed experiences. 将主动搜索他们以前听过的项目，并产生详细的体验</p><p>(3) Critic: the users with this feature demands high standards for items, and may criticize both the recommendation system and the items.</p><p>(4) Chatter: the users with this feature will always engage in private conversations, and trust friends’ recommendations. 将始终参与私人对话，并相信朋友的推荐</p><p>(5) Poster: the users with this feature enjoy publicly posting on social media and sharing content and insights with his friends.喜欢在社交媒体上公开发帖和分享内容）</p><ul><li><p>memory：感觉记忆、短期记忆和长期记忆</p><ul><li><p>感觉记忆：Two individuals, David Smith and David Miller, engage in a conversation about their shared passion for mind-blowing movies, discussing and recommending films such as Interstellar, Inception, The Matrix, Blade Runner, and The Prestige, ultimately planning a movie night and inviting others to join them for a movie marathon.</p></li><li><p>短期记忆：</p><ul><li><p>“Prompt: There are some memories {MR, OB}. Can you infer from the above memories the high-level insight for this person’s character? The insight needs to be significantly different from the content and structure of the original memories. Respond in one sentence. Response in one line.” </p></li><li><p>“Insight: David Miller is a curious and open-minded individual who actively seeks recommendations and discussions about mind-bending movies.”</p></li></ul></li><li><p>长期记忆：</p></li></ul></li><li><p>action：</p><ul><li><p>推荐系统内：(1)搜索行为：通过这些行为，代理可以主动搜索感兴趣的项目。（2）浏览行为：通过这些行为，代理可以被动地接收来自系统的推荐。（3）点击行为：通过这些行为，代理可以选择他们想要观看&#x2F;购买的项目。（4）下一页行为：当用户对当前推荐&#x2F;搜索的项目不满意，并希望看到更多结果时，会触发这些行为。</p></li><li><p>推荐系统之外的行为：（社交网络）实际上，在推荐系统中可能有大量的外部因素影响用户行为。我们认为社会影响可能是最重要的因素之一，例如，真实用户总是相互影响，人们很容易受到商品广告的影响。从不同的社会行为中，我们抽象出两种一般模式：（1）一对一聊天，两个用户讨论和交换他们的信息。这种模式包括用户行为，如通过Twitter、微信等在线聊天，或在咖啡店离线聊天。这些行为可能会触发用户与讨论的项目进行交互，或者改变他们的记忆以影响后续行动。（2）一对多发布，其中一个用户与其他用户共享信息。这种模式包括在社交媒体上发表意见或发送商业广告等用户行为。这些行为可能影响接收共享信息的用户的记忆和行动。</p></li></ul></li></ul><p>引入社交网络，多个agent互动–&gt; 搜索query补足</p><p>记忆体系：感知、短期、长期</p><h4 id="Recommender-AI-Agent-Integrating-Large-Language-Models-for-Interactive-Recommendations"><a href="#Recommender-AI-Agent-Integrating-Large-Language-Models-for-Interactive-Recommendations" class="headerlink" title="Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"></a>Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations</h4><p><strong>对话式推荐系统</strong></p><ul><li><p>提出了InteRecAgent，一个紧凑的框架，通过将LLMs与三组传统推荐模型整合，实现了对话式推荐系统的民主化。</p></li><li><p>为了确保LLMs和推荐系统的有效结合，引入了InteRecAgent内的共享内存总线、动态演示增强的先规划后执行策略和反思策略。</p></li></ul><p>用户通过自然语言与LLMs交互，LLMs解释用户意图并判断当前对话是否需要工具的帮助。在轻松对话中，LLMs基于自身知识回应，而在领域内的推荐任务中，LLMs通过调用一系列工具API并观察工具执行结果生成回应。</p><p>作者提出的最小工具集涵盖了以下方面：</p><ol><li><p><strong>信息查询（Information Query）：</strong> 在对话互动中，InteRecAgent不仅要处理物品推荐任务，还要频繁应对用户的查询。作者引入了一个LLMs配备的项目信息查询模块，通过结构化查询语言（SQL）表达式，从后端项目信息数据库中高效检索详细的项目信息。<br> 例如，在游戏平台上，用户可能会问类似于“这个游戏的发布日期是什么时候？价格是多少？”的问题。为了处理这些查询，LLMs被配置了一个Item Information Query模块，可以通过结构化查询语言（SQL）表达式高效地从后端物品信息数据库中检索详细的物品信息。</p></li><li><p><strong>项目检索（Item Retrieval）：</strong> 用于提出在当前对话中满足用户意图的项目候选列表。工具分为两种类型：硬条件和软条件。硬条件是对项目的明确要求，比如“我想要一些受欢迎的体育游戏”或“推荐我一些价格低于100美元的RPG游戏”。软条件涉及无法用离散属性明确表达的需求，需要使用语义匹配模型，比如“我想要一些类似于《使命召唤》和《堡垒之夜》的游戏”。为了满足这两种条件，InteRecAgent使用SQL工具处理硬条件，从项目数据库中找到候选项目；对于软条件，采用项目到项目工具，基于潜在嵌入匹配相似项目。</p></li><li><p><strong>项目排序（Item Ranking）：</strong> 在对话中，通过分析用户的历史数据和对话中表达的偏好，排名工具对用户进行个性化推荐。排名模块设计为分析用户的历史和对话中提到的具体兴趣，将这些信息作为输入，优先考虑候选集合中的项目。通过有效地对项目进行排序，InteRecAgent可以提供更引人入胜和令人满意的用户体验，从而增强推荐系统的整体效果。</p></li></ol><p>例：</p><p>当用户询问“我已经玩过Fortnite和Call of Duty，现在我想玩一些在Fortnite之后发布的益智游戏，你有什么推荐？”时，工具的执行路径如下：</p><ol><li><ol><li><p><strong>信息查询（Information Query）：</strong> 首先，InteRecAgent通过LLMs解析用户的意图。LLMs会理解用户提到了已经玩过的游戏，以及他们想尝试一些在Fortnite之后发布的益智游戏。在这一阶段，InteRecAgent的LLMs可能使用信息查询工具，通过SQL表达式从后端项目信息数据库中高效检索有关这些游戏的详细信息。</p></li><li><p><strong>项目检索（Item Retrieval）：</strong> 接下来，由于用户表达了明确的需求，即在Fortnite之后发布的益智游戏，InteRecAgent使用SQL检索工具处理硬性条件，从项目数据库中找到符合条件的候选项。此时，候选项将通过Candidate Memory Bus传递，确保不会将所有项附加到提示输入中。</p></li><li><p><strong>项目排名（Item Ranking）：</strong> 最后，InteRecAgent使用排名工具对这些候选项进行排序，考虑用户在对话中提到的历史数据和兴趣。这确保推荐不仅符合用户当前的意图，还符合他们的整体偏好和口味。排序后的结果将成为LLMs生成响应的观察输入，以提供更有吸引力和令人满意的用户体验。</p></li></ol></li></ol><h4 id="On-Generative-Agents-in-Recommendation"><a href="#On-Generative-Agents-in-Recommendation" class="headerlink" title="On Generative Agents in Recommendation"></a>On Generative Agents in Recommendation</h4><p>（i）用户配置模块：用户配置模块包含两个组成部分，一是社会特征，二是独特的品味</p><p>社会特征包含三个关键特征，活跃性，一致性和多样性，活跃性就代表了用户和产品交互的频率和广度，一致性代表了用户的评分这些和平均情况的差异，多样性反映了用户对各种电影类型的倾向，是否是专注于一个方向。</p><p>用户独特品味：</p><p>（ii）内存模块：该模块主要是存储用户的“记忆”，本文将记忆划分为两类，一种是事实记忆，另一种是情感记忆。</p><p>事实记忆是为推荐任务量身定制的，它封装了推荐系统中的交互行为，</p><p>而情感记忆则捕捉了源于这些交互的心理感受。</p><p>具体来说，事实记忆主要包含推荐电影列表，以及用户反馈。反馈涵盖了用户是否观看电影、他相应的评级以及潜在的退出行为等方面。另一方面，情绪记忆记录了用户在系统交互过程中的感受，如疲劳程度和总体满意度。我们的目标是确保生成代理不仅基于过去的事实互动做出反应，而且考虑到感觉，从而更接近地反映真实的人类行为。</p><p>（iii）动作模块：该模块负责用户的行动，一种是受到用户偏好驱动的，一种是受到用户情感驱动的。</p><p>用户偏好驱动</p><p>情感驱动：疲劳度，退出推荐系统后追访。</p><h2 id="基础属性"><a href="#基础属性" class="headerlink" title="基础属性"></a>基础属性</h2><p>——通过前序prompt固定提供</p><p>年龄、性别</p><p>推荐整体满意程度：点展比均值，高、中、低</p><p>探索欲：点击资源的类别数&#x2F;点击资源数，高、中、低</p><p>从众性：点击资源平均分发量</p><p>活跃程度：高活、中活、低活、沉默</p><p>资源质量偏好：点击资源质量分均值</p><p>资源类别偏好</p><h2 id="“记忆”"><a href="#“记忆”" class="headerlink" title="“记忆”"></a>“记忆”</h2><p>——需要时查表</p><p>（ubs_feed.feed_dim_user_dau_shoubai_expand_di）</p><p>城市、资产状况、星座、消费水平、手机型号、教育水平、兴趣偏好、职业、人生阶段、婚姻状态</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>推荐</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
