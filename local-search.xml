<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>uplift与因果推断</title>
    <link href="/2024/07/30/uplift%E4%B8%8E%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD/"/>
    <url>/2024/07/30/uplift%E4%B8%8E%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD/</url>
    
    <content type="html"><![CDATA[<p>在通常的预测任务中，我们拟合的实际是Y与X的相关关系，X甚至可以是Y的结果，如GDP和发电量之间可能有一系列复杂的关系，但只要二者相关就可以互相预测。在另一些场景中则有所区别，如预测任务要指导干预(Treatment)决策时，我们所能掌控的只有Treatment变量，此时我们希望知道的是执行干预与否的效果差异(通常看增量，uplift)，目的是决策是否执行或执行何种干预。如在“发券&amp;下单”的问题中，用户的历史订单数对下单率预估有较大帮助，但对是否发券的指导意义可能会大打折扣。</p><h2 id="相关、因果、辛普森悖论"><a href="#相关、因果、辛普森悖论" class="headerlink" title="相关、因果、辛普森悖论"></a>相关、因果、辛普森悖论</h2><p>因果关系要求“原因”先于并导致“结果”，而相关关系对顺序不做要求。参考材料中提到了很多示例，如“溺水死亡人数与冰激凌销量正相关”，显然二者不是因果关系，而是由“气温(或季节)”联系起来的相关关系。<br><strong>辛普森悖论(Simpson Paradox)</strong><br>趋势出现在几组数据中，但当这些组被合并后趋势消失或反转。<br>案例：<br>总共的志愿者有700个人（相当于小白鼠），分为两个组，第一组给350个人服用新生产的药物，第二组给另外350个人不用药物（或者说服用糖之类的东西，俗称安慰剂）。服药的第一组350个人中，男性患者87位，女性患者350-87 &#x3D;263位。未服药的第二组350个人中，男性患者270位，女性患者350-270&#x3D;80位，实验结果如表所示<br><img src="https://obsidian-1319504291.cos.ap-beijing.myqcloud.com/20240730165623.png" alt="image.png"><br>总数据中，服药的患者痊愈率是78%，未服药的患者痊愈率是83%<br>但是在服药的患者中，男性患者的痊愈率是93%，而未服药的男性患者痊愈率是87%，证明药物有效；女性服药的患者痊愈率是73%，而未服药的患者痊愈率是69%，药物同样有效。<br>整体的效果而言，竟然是不服药的效果好。<br><strong>从数据的角度来说</strong>，此次服用药物患者的350人和不服用药物的350人中，男女比例是不一样的。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>728</title>
    <link href="/2024/07/28/0728/"/>
    <url>/2024/07/28/0728/</url>
    
    <content type="html"><![CDATA[<p>0728test</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>预训练语言模型</title>
    <link href="/2024/07/19/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    <url>/2024/07/19/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h3 id="NLP语言模型发展一览"><a href="#NLP语言模型发展一览" class="headerlink" title="NLP语言模型发展一览"></a>NLP语言模型发展一览</h3><p>深度学习阶段：encoder-decoder&#x2F;word2vec&#x2F;attention<br>预训练模型阶段：transformer&#x2F;bert&#x2F;GPT-2&#x2F;GPT-3，pre- training+fine-tuning<br>大语言模型阶段：GPT3.5&#x2F;GPT4，instruction-tuning&#x2F;prompt-training</p><pre><code class="hljs">深度学习的训练：inference得到预测值与标注数据对比，使用loss function得到差，使用back propagation修改。过程中进行梯度下降（SGD）</code></pre><p>预训练语言模型首先使用大量未标注数据进行自监督学习，学习出的语言模型针对特定的下游任务进行微调。</p><h3 id="prompt-tuning和fine-tuning的区别"><a href="#prompt-tuning和fine-tuning的区别" class="headerlink" title="prompt tuning和fine tuning的区别"></a>prompt tuning和fine tuning的区别</h3><p>prompt tuning会固定预训练模型的参数<br>Prompt Tuning和Fine Tuning虽然都是微调，但是有区别。提示学习(PromptTuning)的灵感源于GPT-3,就是在Prompt中插入一段task specific的可以tune的prompt token。由于这个token对于每个任务都是不同的，所以可以帮助机器识别任务到底是什么。又因为机器自己学习(tune)这个prompt token,所以这个token对于机器会有非常好的效果。</p><p>提出Prompt Tuning的动机是，语言模型（Language Models）越来越大，Fine-tune的成本也越来越高。<br>Fine-tuning的本质是改变预训练模型的weights。由于预训练模型在原域上已经有非常好的性能了，域的迁移会受到原域的阻力，因为使用fine-tune改变的weight是原域上的weight。这样，想要在新域上获得较好的性能，预训练模型越大，fine-tune需要的数据也越多。</p><h4 id="prompt的分类"><a href="#prompt的分类" class="headerlink" title="prompt的分类"></a>prompt的分类</h4><h5 id="硬提示-离散提示（Hard-Prompt-Discrete-Prompt）"><a href="#硬提示-离散提示（Hard-Prompt-Discrete-Prompt）" class="headerlink" title="硬提示&#x2F;离散提示（Hard Prompt&#x2F;Discrete Prompt）"></a>硬提示&#x2F;离散提示（Hard Prompt&#x2F;Discrete Prompt）</h5><p>人为设计<br>根据2021年的两份研究，硬提示有两个性质：</p><ul><li>人类认为不错的硬提示对于LM来说不一定是一个好的硬提示，这个性质被称为硬提示的sub-optimal（次优）性。</li><li>硬提示的选择对于预训练模型的影响非常大。</li></ul><h5 id="软提示-连续提示（Soft-Prompt-Continuous-Prompt）"><a href="#软提示-连续提示（Soft-Prompt-Continuous-Prompt）" class="headerlink" title="软提示&#x2F;连续提示（Soft Prompt&#x2F;Continuous Prompt）"></a>软提示&#x2F;连续提示（Soft Prompt&#x2F;Continuous Prompt）</h5><p>把Prompt的生成本身作为一个任务进行学习，相当于把Prompt的生成从人类一个一个尝试（离散）变换成机器自己进行学习、尝试（连续）。由于需要机器自己学习，软提示不可避免地往模型内引入了新的参数。这里就又出来一个问题：如何<strong>参数有效</strong>地学习软提示？目前的研究热点有：</p><ul><li>P-tuning：将prompt变成token，用BiLSTM进行学习。</li><li>P-tuning：使用混合的prompt初始化策略（如CLInit和SelectInit）。</li><li>Prefix-tuning：对于不同模型，将prompt注入输入的不同位置。</li></ul><h4 id="从Prompt到Prompt-Tuning：让机器学习写Prompt"><a href="#从Prompt到Prompt-Tuning：让机器学习写Prompt" class="headerlink" title="从Prompt到Prompt Tuning：让机器学习写Prompt"></a>从Prompt到Prompt Tuning：让机器学习写Prompt</h4><p><strong><em>所谓Prompt Tuning，就是在Prompt中插入一段task-specific的可以tune的prompt token。</em></strong><br>由于这个token对于每个任务都是不同的，所以可以帮助机器识别任务到底是什么。又因为机器自己学习（tune）这个prompt token，所以这个token对于机器会有非常好的效果。</p><h5 id="参数有效性（Parameter-efficiency）"><a href="#参数有效性（Parameter-efficiency）" class="headerlink" title="参数有效性（Parameter-efficiency）"></a>参数有效性（Parameter-efficiency）</h5><p>所谓“参数有效”，本质就是“节约参数”。怎样算节约参数呢？复用预训练模型越多，节约的参数越多。这是因为Fine-tune预训练模型相当于改变整个预训练模型，而这需要改变非常多的参数。想要进行Transfer Learning的话，改变这么多参数是不能接受的。因此，Prompt Learning的一大好处就叫Parameter Efficient。</p><h3 id="预训练模型三种网络架构"><a href="#预训练模型三种网络架构" class="headerlink" title="预训练模型三种网络架构"></a>预训练模型三种网络架构</h3><p>1.Encoders编码器：编码器主要用于处理和理解输入信息。这些模型可以获得双向的上下文，即可以同时考虑一个词的前面和后面的信息。由于编码器可以考虑到未来的信息，它们非常适合用于需要理解整个句子的任务，如文本分类、命名实体识别等。预训练编码器需要使其能够构建强大的表示，这通常通过预测某个被遮蔽的单词、预测下一个句子或者其它的预训练任务来实现。BERT就是一种典型的预训练编码器，如RoBERTa、ALBERT。百度的ERNIE也跟BERT类似。<br>2.Decoders解码器：解码器主要用于生成输出信息。它们是语言模型的基础，用于预测下一个单词。解码器只能考虑到过去的词，而不能考虑到未来的词，这使得它们非常适合于生成任务，如文本生成、对话系统等。GPT就是一种典型的预训练解码器。如GPT-1、GPT-2、GPT-3三代。所以说GPT名称叫作预训练的解码器也是可以的。<br>3.Encoders-Decoders编码器-解码器：编码器-解码器结构结合了编码器和解码器的优点。编码器首先处理输入信息，然后解码器生成输出信息。这种结构可以考虑到全局的上下文信息，因此非常适合于需要理解输入信息并生成相应的Decoders输出的任务，如机器翻译、文本摘要等。如何预训练编码器解码器模型是一个开放的问题，因为需要考虑到如何有效地使用编码器和解码器的特点。T5和BART都是典型的预训练编码器解码器模型。</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
